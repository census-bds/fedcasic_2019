{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rapidly Prototyping a Machine Learning Pipeline\n",
    "\n",
    "The purpose of this workshop is to show how easy it is to take an idea and turn it into a successful Machine Learning application.\n",
    "\n",
    "**Front Matter**: See the [README.md](README.md) file for environment setup concerns. This is a [Jupyter Notebook](https://jupyter.org/) -- an interactive computing environment that's popular in the Python community. Essentially, we can run and re-run cells of code to iteratively explore a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this cell will (ideally) install NLTK\n",
    "# and download the english stopwords.\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 0\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# this is a trick to parallelize our computations\n",
    "import multiprocessing\n",
    "NCPUS = 8 if multiprocessing.cpu_count() > 8 else (multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: The idea/loading data\n",
    "In line with Tuesday's panel around \"autocoders,\" we'll demonstrate how to use text descriptions to predict codes. Along the way, we'll show some good practice when practicing machine learning. While this notebook will focus on coding products using descriptions, this approach should work for many scenarios where one has a dataset with both text descriptions and their associated codes.\n",
    "\n",
    "#### The data\n",
    "This data comes from Census' own Foreign Trade site: https://www.census.gov/foreign-trade/schedules/b/index.html#download\n",
    "\n",
    "Specifically, this is the \"concordance file\" for the Harmonized System's import/export codes. This gives us the code and a description of what products fit into that code: perfect for the sort of automatic coding we want to do. We've included the file here (in the `data/` subfolder) for ease-of-use.\n",
    "\n",
    "The import and export codes are slightly different at the 10-digit level; however, the codes are heirarchical and at the 6-digit level they are the same (as defined by an international standards group). We'll model at an even less granular level than that - the 4-digit level - based on the amount of data that we have.\n",
    "\n",
    "First, let's take a look in a text editor. Jupyter has one that's enough for this.\n",
    "\n",
    "From here, taking a look at the `imp-stru.txt` file will give us the \"schema.\"\n",
    "\n",
    "For the purposes of this workshop, we'll aim to manipulate data exclusively in [Pandas](https://pandas.pydata.org/) -- a fully-featured and quite powerful tabular data analysis library for Python -- to keep things more consistent. Howevever, you could just as easily do this in a combination of \"pure python\" + Pandas.\n",
    "\n",
    "First, let's load up one of the files and see what it looks like in Pandas. In general, Pandas makes it easy to read in common file types such as `csv` and `xlsx`. In this case, the file type is slightly different, so we'll have to manually process based on the schema we've been given. Luckily, this is pretty easy in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0101210010    HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE        HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                            NO              00150     12060     0    112920     00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101210020    HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE      HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                          NO              00150     12060     0    112920     00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101290010    HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI           HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                   NO              00150     00100     0    112920     00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0101290090    HORSES, LIVE, NESOI                                    HORSES, LIVE, NESOI                                                                                                                                        NO              00150     12060     0    112920     00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101300000    ASSES, LIVE                                            ASSES, LIVE                                                                                                                                                NO              00150     12060     0    112920     00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101903000    MULES AND HINNIES, FOR IMMEDIATE SLAUGHTER, LIVE       MULES AND HINNIES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE                                                                                                  NO              00150     00100     0    112920     00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0101210010    HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE        HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                            NO              00150     12060     0    112920     00\n",
       "0  0101210020    HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE      HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                          NO              00150     12060     0    112920     00\n",
       "1  0101290010    HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI           HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                   NO              00150     00100     0    112920     00\n",
       "2  0101290090    HORSES, LIVE, NESOI                                    HORSES, LIVE, NESOI                                                                                                                                        NO              00150     12060     0    112920     00\n",
       "3  0101300000    ASSES, LIVE                                            ASSES, LIVE                                                                                                                                                NO              00150     12060     0    112920     00\n",
       "4  0101903000    MULES AND HINNIES, FOR IMMEDIATE SLAUGHTER, LIVE       MULES AND HINNIES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE                                                                                                  NO              00150     00100     0    112920     00"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here, we will use the general 'read_table' convenience function.\n",
    "# however, two more common ones are 'pd.read_csv' and 'pd.read_xlsx'\n",
    "df1 = pd.read_table(\"data/imp-code.txt\")\n",
    "\n",
    "# an easy way to show the first few rows of a data set.\n",
    "# by default, the Jupyter Notebook shows the output of the last command you ran.\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101210010    HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE        HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                            NO              00150     12060     0    112920     00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210020    HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE      HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                          NO              00150     12060     0    112920     00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0101290010    HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI           HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                   NO              00150     00100     0    112920     00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101290090    HORSES, LIVE, NESOI                                    HORSES, LIVE, NESOI                                                                                                                                        NO              00150     12060     0    112920     00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101300000    ASSES, LIVE                                            ASSES, LIVE                                                                                                                                                NO              00150     12060     0    112920     00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                 all_text\n",
       "0  0101210010    HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE        HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                            NO              00150     12060     0    112920     00\n",
       "1  0101210020    HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE      HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                          NO              00150     12060     0    112920     00\n",
       "2  0101290010    HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI           HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                   NO              00150     00100     0    112920     00\n",
       "3  0101290090    HORSES, LIVE, NESOI                                    HORSES, LIVE, NESOI                                                                                                                                        NO              00150     12060     0    112920     00\n",
       "4  0101300000    ASSES, LIVE                                            ASSES, LIVE                                                                                                                                                NO              00150     12060     0    112920     00"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first row is incorrectly inferred to be the header. This is easy\n",
    "# to fix with a parameter to the read_table function\n",
    "df1 = pd.read_table(\"data/imp-code.txt\", header=None, names=['all_text'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "      <th>HS10</th>\n",
       "      <th>long_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101210010    HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE        HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                            NO              00150     12060     0    112920     00</td>\n",
       "      <td>0101210010</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210020    HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE      HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                          NO              00150     12060     0    112920     00</td>\n",
       "      <td>0101210020</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0101290010    HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI           HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                   NO              00150     00100     0    112920     00</td>\n",
       "      <td>0101290010</td>\n",
       "      <td>HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101290090    HORSES, LIVE, NESOI                                    HORSES, LIVE, NESOI                                                                                                                                        NO              00150     12060     0    112920     00</td>\n",
       "      <td>0101290090</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101300000    ASSES, LIVE                                            ASSES, LIVE                                                                                                                                                NO              00150     12060     0    112920     00</td>\n",
       "      <td>0101300000</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                 all_text  \\\n",
       "0  0101210010    HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE        HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                            NO              00150     12060     0    112920     00   \n",
       "1  0101210020    HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE      HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                          NO              00150     12060     0    112920     00   \n",
       "2  0101290010    HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI           HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                   NO              00150     00100     0    112920     00   \n",
       "3  0101290090    HORSES, LIVE, NESOI                                    HORSES, LIVE, NESOI                                                                                                                                        NO              00150     12060     0    112920     00   \n",
       "4  0101300000    ASSES, LIVE                                            ASSES, LIVE                                                                                                                                                NO              00150     12060     0    112920     00   \n",
       "\n",
       "         HS10  \\\n",
       "0  0101210010   \n",
       "1  0101210020   \n",
       "2  0101290010   \n",
       "3  0101290090   \n",
       "4  0101300000   \n",
       "\n",
       "                                                                                                                                                long_desc  \n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                         \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                       \n",
       "2  HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                \n",
       "3  HORSES, LIVE, NESOI                                                                                                                                     \n",
       "4  ASSES, LIVE                                                                                                                                             "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we only have 1 column. \n",
    "# we want to break this out into separate columns based on our schema. \n",
    "# this is easy to do with Pandas...\n",
    "df1[\"HS10\"] = df1[\"all_text\"].str[:10]\n",
    "df1[\"long_desc\"] = df1[\"all_text\"].str[69:219]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# great. Now let's get rid of the initial column\n",
    "# since we've put all we care about into separate columns.\n",
    "del df1[\"all_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS10</th>\n",
       "      <th>long_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101210010</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210020</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0101290010</td>\n",
       "      <td>HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101290090</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101300000</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         HS10  \\\n",
       "0  0101210010   \n",
       "1  0101210020   \n",
       "2  0101290010   \n",
       "3  0101290090   \n",
       "4  0101300000   \n",
       "\n",
       "                                                                                                                                                long_desc  \n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                         \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                       \n",
       "2  HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                \n",
       "3  HORSES, LIVE, NESOI                                                                                                                                     \n",
       "4  ASSES, LIVE                                                                                                                                             "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last, let's repeat this for the other file.\n",
    "\n",
    "df2 = pd.read_table(\"data/exp-code.txt\", header=None, names=['all_text'])\n",
    "df2[\"HS10\"] = df2[\"all_text\"].str[:10]\n",
    "df2[\"long_desc\"] = df2[\"all_text\"].str[69:219]\n",
    "del df2[\"all_text\"]\n",
    "\n",
    "# and we have our final output\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at some metadata..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df has 28469 rows\n",
      "df has 22660 unique HS 10-digit codes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS10</th>\n",
       "      <th>long_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101210010</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210020</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0101290010</td>\n",
       "      <td>HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101290090</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101300000</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         HS10  \\\n",
       "0  0101210010   \n",
       "1  0101210020   \n",
       "2  0101290010   \n",
       "3  0101290090   \n",
       "4  0101300000   \n",
       "\n",
       "                                                                                                                                                long_desc  \n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                         \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                       \n",
       "2  HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                \n",
       "3  HORSES, LIVE, NESOI                                                                                                                                     \n",
       "4  ASSES, LIVE                                                                                                                                             "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"df has\", len(df), \"rows\")\n",
    "print(\"df has\", df['HS10'].nunique(), 'unique HS 10-digit codes')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. We have something like 28,000 rows. As we can immediately see, there is some overlap between the 10-digit import and export codes, hence the only ~22,000 unique 10-digit codes. It is likely that duplicate codes have the same description in both files... and from the perspective of the model, 2 copies of the same description/code is just as good as 1, so we'll only want to retain one copy of those. Additionally, we need to standardize and process this text, which will probably leave us with more of duplicate text strings as well. Let's start processing and find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Cleaning / processing the data\n",
    "\n",
    "There are a number of popular libraries for processing text in Python. In this case, we'll use `nltk`, but for reference `spaCy` is another very good option.\n",
    "\n",
    "Additionally, depending on the type of text that you have, there are many different ways to process and \"extract features\" (i.e. create variables for modelling) from that text. For example, if you're working with phrases/sentences, spaCy has good tools for determining which part-of-speech each word in a sentence maps to.\n",
    "\n",
    "In our case, we have very simple product descriptions (perhaps closer to \"tags\" than sentences). Plus, they have been officially written and are relatively standardized. As a result, less processing is required. We'll start out by using regular expressions. This is a good tutorial to learn more about those: https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial\n",
    "\n",
    "We could also use NLTK's \"tokenizer\", but this is overkill for such simple descriptions.\n",
    "\n",
    "Last, we'll show off the same common workflow we began above - using one column to derive another, processed column in Pandas. [This link](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html) shows off some of Pandas' built-in string processing functionality -- essentially, everything you an access by using `df[col_name].str` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS10</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>long_stripped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13946</th>\n",
       "      <td>7409290050</td>\n",
       "      <td>BRASS PLATES, SHEET AND STRIP, NOT COILED, THCKNESS OF OVER 0.15 BUT LESS THAN 5MM, WITH WIDTH OF 500MM OR M0RE</td>\n",
       "      <td>BRASS PLATES, SHEET AND STRIP, NOT COILED, THCKNESS OF OVER 0.15 BUT LESS THAN 5MM, WITH WIDTH OF 500MM OR M0RE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27426</th>\n",
       "      <td>8540110080</td>\n",
       "      <td>CATHODE-RAY TELEVISION PICTURE TUBES, COLOR, HAVING A VIDEO DISPLAY DIAGONAL EXCEEDING 67 CM (26 INCHES)</td>\n",
       "      <td>CATHODE-RAY TELEVISION PICTURE TUBES, COLOR, HAVING A VIDEO DISPLAY DIAGONAL EXCEEDING 67 CM (26 INCHES)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26184</th>\n",
       "      <td>8436100000</td>\n",
       "      <td>MACHINERY FOR PREPARING ANIMAL FEEDS</td>\n",
       "      <td>MACHINERY FOR PREPARING ANIMAL FEEDS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             HS10  \\\n",
       "13946  7409290050   \n",
       "27426  8540110080   \n",
       "26184  8436100000   \n",
       "\n",
       "                                                                                                                                                    long_desc  \\\n",
       "13946  BRASS PLATES, SHEET AND STRIP, NOT COILED, THCKNESS OF OVER 0.15 BUT LESS THAN 5MM, WITH WIDTH OF 500MM OR M0RE                                          \n",
       "27426  CATHODE-RAY TELEVISION PICTURE TUBES, COLOR, HAVING A VIDEO DISPLAY DIAGONAL EXCEEDING 67 CM (26 INCHES)                                                 \n",
       "26184  MACHINERY FOR PREPARING ANIMAL FEEDS                                                                                                                     \n",
       "\n",
       "                                                                                                         long_stripped  \n",
       "13946  BRASS PLATES, SHEET AND STRIP, NOT COILED, THCKNESS OF OVER 0.15 BUT LESS THAN 5MM, WITH WIDTH OF 500MM OR M0RE  \n",
       "27426  CATHODE-RAY TELEVISION PICTURE TUBES, COLOR, HAVING A VIDEO DISPLAY DIAGONAL EXCEEDING 67 CM (26 INCHES)         \n",
       "26184  MACHINERY FOR PREPARING ANIMAL FEEDS                                                                             "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll do this step-by-step, to illustrate\n",
    "df['long_stripped'] = df['long_desc'].str.strip()\n",
    "\n",
    "# this is pulling 3 random rows, instead of the top 5, just for some variety\n",
    "# note the new colummn\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS10</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>long_stripped</th>\n",
       "      <th>long_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101210010</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>horses and asses, purebred breeding, male, live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210020</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>horses and asses, purebred breeding, female, live</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         HS10  \\\n",
       "0  0101210010   \n",
       "1  0101210020   \n",
       "\n",
       "                                                                                                                                                long_desc  \\\n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                          \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                        \n",
       "\n",
       "                                       long_stripped  \\\n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE     \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE   \n",
       "\n",
       "                                          long_lower  \n",
       "0  horses and asses, purebred breeding, male, live    \n",
       "1  horses and asses, purebred breeding, female, live  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['long_lower'] = df['long_stripped'].str.lower()\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['horses', 'and', 'asses', 'purebred', 'breeding', 'male', 'live']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# this is in general one of the more useful regular expressions to know\n",
    "# the '\\w' searches for word-like tokens, and the '+' says \"one or more\"\n",
    "# combined, this gets us 'one or more characters', i.e. word tokens w/o commas, spaces, etc.\n",
    "WORD_REGEX = r'\\w+'\n",
    "\n",
    "\n",
    "# first, to illustrate\n",
    "re.findall(WORD_REGEX, \"horses and asses, purebred breeding, male, live\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS10</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>long_stripped</th>\n",
       "      <th>long_lower</th>\n",
       "      <th>long_word_list</th>\n",
       "      <th>long_words_only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101210010</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>horses and asses, purebred breeding, male, live</td>\n",
       "      <td>[horses, and, asses, purebred, breeding, male, live]</td>\n",
       "      <td>horses and asses purebred breeding male live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210020</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>horses and asses, purebred breeding, female, live</td>\n",
       "      <td>[horses, and, asses, purebred, breeding, female, live]</td>\n",
       "      <td>horses and asses purebred breeding female live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0101290010</td>\n",
       "      <td>HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING</td>\n",
       "      <td>HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING</td>\n",
       "      <td>horses, imported for immediate slaughter, live, except purebred breeding</td>\n",
       "      <td>[horses, imported, for, immediate, slaughter, live, except, purebred, breeding]</td>\n",
       "      <td>horses imported for immediate slaughter live except purebred breeding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101290090</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "      <td>horses, live, nesoi</td>\n",
       "      <td>[horses, live, nesoi]</td>\n",
       "      <td>horses live nesoi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101300000</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "      <td>asses, live</td>\n",
       "      <td>[asses, live]</td>\n",
       "      <td>asses live</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         HS10  \\\n",
       "0  0101210010   \n",
       "1  0101210020   \n",
       "2  0101290010   \n",
       "3  0101290090   \n",
       "4  0101300000   \n",
       "\n",
       "                                                                                                                                                long_desc  \\\n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                          \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                        \n",
       "2  HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                 \n",
       "3  HORSES, LIVE, NESOI                                                                                                                                      \n",
       "4  ASSES, LIVE                                                                                                                                              \n",
       "\n",
       "                                                              long_stripped  \\\n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                            \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                          \n",
       "2  HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING   \n",
       "3  HORSES, LIVE, NESOI                                                        \n",
       "4  ASSES, LIVE                                                                \n",
       "\n",
       "                                                                 long_lower  \\\n",
       "0  horses and asses, purebred breeding, male, live                            \n",
       "1  horses and asses, purebred breeding, female, live                          \n",
       "2  horses, imported for immediate slaughter, live, except purebred breeding   \n",
       "3  horses, live, nesoi                                                        \n",
       "4  asses, live                                                                \n",
       "\n",
       "                                                                    long_word_list  \\\n",
       "0  [horses, and, asses, purebred, breeding, male, live]                              \n",
       "1  [horses, and, asses, purebred, breeding, female, live]                            \n",
       "2  [horses, imported, for, immediate, slaughter, live, except, purebred, breeding]   \n",
       "3  [horses, live, nesoi]                                                             \n",
       "4  [asses, live]                                                                     \n",
       "\n",
       "                                                         long_words_only  \n",
       "0  horses and asses purebred breeding male live                           \n",
       "1  horses and asses purebred breeding female live                         \n",
       "2  horses imported for immediate slaughter live except purebred breeding  \n",
       "3  horses live nesoi                                                      \n",
       "4  asses live                                                             "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there's a fair bit going on here. \n",
    "# first, we define a function that works on an individual\n",
    "# description.\n",
    "def find_words(desc):\n",
    "    return re.findall(WORD_REGEX, desc)\n",
    "\n",
    "# then, we 'apply' that function to a column, which \n",
    "# means that we call this function on every cell within\n",
    "# the column.\n",
    "df['long_word_list'] = df['long_lower'].apply(find_words)\n",
    "\n",
    "# now, combine the list of words back into a clean string\n",
    "df['long_words_only'] = df['long_word_list'].str.join(' ')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean things up a bit, let's get rid of some of these intermediate columns.\n",
    "\n",
    "In practice, this is a useful thing to do because it will free up memory. If you find your code is using too much memory or is running very slow, then consider deleting unnecessary columns in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS10</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>long_words_only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101210010</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>horses and asses purebred breeding male live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210020</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>horses and asses purebred breeding female live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0101290010</td>\n",
       "      <td>HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING</td>\n",
       "      <td>horses imported for immediate slaughter live except purebred breeding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101290090</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "      <td>horses live nesoi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101300000</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "      <td>asses live</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         HS10  \\\n",
       "0  0101210010   \n",
       "1  0101210020   \n",
       "2  0101290010   \n",
       "3  0101290090   \n",
       "4  0101300000   \n",
       "\n",
       "                                                                                                                                                long_desc  \\\n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                          \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                        \n",
       "2  HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                 \n",
       "3  HORSES, LIVE, NESOI                                                                                                                                      \n",
       "4  ASSES, LIVE                                                                                                                                              \n",
       "\n",
       "                                                         long_words_only  \n",
       "0  horses and asses purebred breeding male live                           \n",
       "1  horses and asses purebred breeding female live                         \n",
       "2  horses imported for immediate slaughter live except purebred breeding  \n",
       "3  horses live nesoi                                                      \n",
       "4  asses live                                                             "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running this cell more than once will cause an error\n",
    "# because when you try to delete somethat that has \n",
    "# already been deleted, it's no longer there\n",
    "del df['long_stripped']\n",
    "del df['long_lower']\n",
    "del df['long_word_list']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Initial Model\n",
    "Now that we've gotten English words only, we can try a simple model. Let's get into the modelling approach and the package that we'll use to implement it, `scikit-learn`. Pandas is a \"batteries-included\" data analysis library -- you can do a large number of data analysis tasks without writing \"pure\" python and exclusively staying within Pandas' programming interface. \n",
    "\n",
    "Somewhat more frequently, scikit-learn requires you to plug in some batteries... but it does play nice with Pandas and overall works very well, however. Furthermore, it is a remarkably comprehensive library that includes a large number of tools for preprocessing your data, selecting and extracting important variables (in the machine learning parlance, \"features\"), and models to try out. Last, it provides a clean and mostly consistent interface, which makes experimenting significantly easier.\n",
    "\n",
    "We'll be implementing a _bag-of-words_ model. The idea is very simple: each word becomes a separate variable. For each variable, the value is the number of times that word occurs in that particular record. Let's demonstrate with a quick example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    horses and asses purebred breeding male live  \n",
       "1    horses and asses purebred breeding female live\n",
       "Name: long_words_only, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try this process on the first two and see what we get back\n",
    "first_few_only = df.head(2)\n",
    "first_few_only['long_words_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn calls this process \"vectorizing\", i.e. turning a sentence into a vector of variables.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "# this will actually convert our first few descriptions into vectors\n",
    "tfd = cv.fit_transform(first_few_only['long_words_only'])\n",
    "# by default, it's a sparse matrix\n",
    "tfd.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For explainability, let's cleanly present this mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>asses</th>\n",
       "      <th>breeding</th>\n",
       "      <th>female</th>\n",
       "      <th>horses</th>\n",
       "      <th>live</th>\n",
       "      <th>male</th>\n",
       "      <th>purebred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_words_only</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>horses and asses purebred breeding male live</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horses and asses purebred breeding female live</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                and  asses  breeding  female  \\\n",
       "long_words_only                                                                \n",
       "horses and asses purebred breeding male live    1    1      1         0        \n",
       "horses and asses purebred breeding female live  1    1      1         1        \n",
       "\n",
       "                                                horses  live  male  purebred  \n",
       "long_words_only                                                               \n",
       "horses and asses purebred breeding male live    1       1     1     1         \n",
       "horses and asses purebred breeding female live  1       1     0     1         "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't worry about this, it's for pedagogical purposes\n",
    "columns = [x[0] for x in sorted(list(cv.vocabulary_.items()), key=lambda x: x[1])]\n",
    "pd.DataFrame(tfd.toarray(), columns=columns, index=first_few_only['long_words_only'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we get the idea, let's do this for the entire dataset and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<28469x12054 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 347471 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit_transform(df[\"long_words_only\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't try to convert this into an array as above, because it would be very memory-intensive. But we can see we have 12,054 unique words.\n",
    "\n",
    "Now, we have features (i.e. input variables). But what what exactly are we going to model? The trade-off is that the more digits of the HS code we use, the fewer records there are within each code that the model can learn from. We aren't using any heirarchical information here, so each HS code is a completely separate category from the model's perspective. Let's extract 2-digit, 4-digit, and 6-digit harmonized codes in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>290.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>411.851396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>85.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>160.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>307.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2904.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               HS2\n",
       "count  98.000000  \n",
       "mean   290.500000 \n",
       "std    411.851396 \n",
       "min    2.000000   \n",
       "25%    85.250000  \n",
       "50%    160.000000 \n",
       "75%    307.500000 \n",
       "max    2904.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEECAYAAADpigmnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAEWJJREFUeJzt3XGsnXV9x/H3V8k6bRVYqwVX2opL\ntiA4MzujQdjt2ugUJkXdoiEhdWqni1tQsgVJRMAR2JJuoNHMOpxuQx3oFJ2KQ9s7hCmmmJnOLZNV\n2ooOq5SKt1iw9rs/znPTh8Nt77nfe88959y+X8mT+3ue5/ec3+8+33A/nOc852lkJpIkVTxp0BOQ\nJI0uQ0SSVGaISJLKDBFJUpkhIkkqM0QkSWWGiCSpzBCRJJUZIpKkshMGPYFjWbZsWa5evbp07IED\nB1i8ePHcTkhzyhoNP2s0/Kaq0T333POjzHzGfIw/1CGyevVqtm/fXjp2fHycsbGxuZ2Q5pQ1Gn7W\naPhNVaOI2D1f43s5S5JUZohIksoMEUlSmSEiSSozRCRJZYaIJKnMEJEklRkikqQyQ0SSVDbU31if\njR3f+zEbL/vcQMbedd15AxlXkuab70QkSWWGiCSpzBCRJJUZIpKkMkNEklRmiEiSygwRSVKZISJJ\nKjNEJEllhogkqcwQkSSVGSKSpDJDRJJUZohIksoMEUlSmSEiSSozRCRJZYaIJKnMEJEklRkikqQy\nQ0SSVGaISJLKDBFJUpkhIkkqM0QkSWU9h0hE/GJEXBIR/x4RD0XEwYjYExG3RcTruvoui4jNEXFv\n029fRNweEefP/a8gSRqUE3rpFBGnAl8Afr1r12nNMgF8rOm7CrgDWNnqtwhYD6yPiCsy892znLck\naQhMGyIREcA/cSRAdgDvA3YCTwPOAA61DrmRIwFyN3Ad8GvANXTe+VwVEVsz8665+AUkSYPTyzuR\nVwDnNO3/Bl6UmY+09n9qshERZwLrmtUEXpOZ9zf7ngO8EQjgEsAQkaQR18tnIq9qtb8B/ENE/F9E\nPBIR2yPi4tb+da327skAabRDY21hrpKkIdPLO5HntdoXde17AfCRiDgjMy8DTm/te6Crb3t9aUSc\nlJn7e5+qJGnY9BIiJ3Wtb6FzCetCYFOz7c8i4u+Bxa1+j3Ud172+BHhCiETEpsnXXb58OePj4z1M\n8YmWPwUuPevQ9B37oDrn483ExITnashZo+E36Br1EiIHW+3vA2/JzMMR8a/A7wKn0vmc43eAA62+\ni7pep3t9YqrBMnMLnaBizZo1OTY21sMUn+i9N93K5h093Xw253ZdNDaQcUfN+Pg41fpqflij4Tfo\nGvXymcjuVntPZh4GaH62950IfKe1fkrX65zaaj/opSxJGn29hMi/tdorI+JJAM3P9ndBdgNbu/q2\n95/bam+b6UQlScOnlxD5CPBw034W8L6IeBmd74o8q9k+AXw2M3dwJCACuCUiNkTE5cDkXVwJ3DAX\nk5ckDda0Hxpk5t6I+APg403/NzfLpEPAmzLzh836G+h8Y30F8EJa3yNpXJ2Zd8524pKkwevp2VmZ\n+UngRcAngL10gmNvs/7izPx4q+99dG79vZ7Ot9ofo3MX1lbggsy8cg7nL0kaoJ5vX8rMe4Df67Hv\nXuBtzSJJWqB8FLwkqcwQkSSVGSKSpDJDRJJUZohIksoMEUlSmSEiSSozRCRJZYaIJKnMEJEklRki\nkqQyQ0SSVGaISJLKDBFJUpkhIkkqM0QkSWWGiCSpzBCRJJUZIpKkMkNEklRmiEiSygwRSVKZISJJ\nKjNEJEllhogkqcwQkSSVGSKSpDJDRJJUZohIksoMEUlSmSEiSSozRCRJZYaIJKnMEJEklRkikqQy\nQ0SSVGaISJLKDBFJUpkhIkkqM0QkSWWGiCSpzBCRJJUZIpKkMkNEklRWCpGIeHlEZGvZNUWfZRGx\nOSLujYiDEbEvIm6PiPNnPWtJ0lA4YaYHRMRS4EPT9FkF3AGsbG1eBKwH1kfEFZn57pmOLUkaLpV3\nIh8ATgEOHqPPjRwJkLuBC4F3AIebbVdFxNmFsSVJQ2RGIRIRFwOvBn4MXHuUPmcC65rVBF6TmZ/O\nzOs48g4mgEtKM5YkDY2eQyQiVgLvaVbfCuw5Std1rfbuzLy/tX5Xq72217ElScOppxCJiAA+ApwI\n3JyZ/3iM7qe32g907WuvL42Ik3qapSRpKPX6wfqlwBjwfeAt0/Rd3Go/1rWve30JsL+9ISI2AZsA\nli9fzvj4eI9TfLzlT4FLzzpUOna2qnM+3kxMTHiuhpw1Gn6DrtG0IRIRvwz8OZ3PN16fmfumOeRA\nq72oa1/3+kT3wZm5BdgCsGbNmhwbG5tuilN67023snnHjG8+mxO7LhobyLijZnx8nGp9NT+s0fAb\ndI16+Sv7DI788f9i58rWE6yKiARuBba1tp/S1e/UVvvBzNyPJGlk9eMb61tb7ZXNB/KTzm2122Ej\nSRpBvbwT+R7wtim2vxB4XdN+CLga2JmZOyJiG527rwK4JSKuBc4ALm76J3DDbCYuSRq8aUMkM38I\nXN+9PSI2ciREHs7Mdp830PnG+go6YfOprsOvzsw7KxOWJA2PvjyAMTPvA15AJ3x20rkraz+dS10X\nZOaV/RhXkjS/yrcvZeaHgQ8fY/9eOpfBproUJklaAHwUvCSpzBCRJJUZIpKkMkNEklRmiEiSygwR\nSVKZISJJKjNEJEllhogkqcwQkSSVGSKSpDJDRJJUZohIksoMEUlSmSEiSSozRCRJZYaIJKnMEJEk\nlRkikqQyQ0SSVGaISJLKDBFJUpkhIkkqM0QkSWWGiCSpzBCRJJUZIpKkMkNEklRmiEiSygwRSVKZ\nISJJKjNEJEllhogkqcwQkSSVGSKSpDJDRJJUZohIksoMEUlSmSEiSSozRCRJZYaIJKnMEJEklRki\nkqQyQ0SSVGaISJLKegqRiHh+RFwTEV+JiD0R8dOIOBAR34yId0XEkimOWRYRmyPi3og4GBH7IuL2\niDh/7n8NSdIgnNBjvzcDfzjF9uc1y+9HxIsz82GAiFgF3AGsbPVdBKwH1kfEFZn57vq0JUnDYCaX\ns/YBNwAbgPOBW1r7zgAuaa3fyJEAuRu4EHgHcLjZdlVEnF2ZsCRpePT6TuSjwJ9m5k8mN0TE54Ff\npfNOBOBFzfYzgXXNtgRek5n3N/ueA7wRCDqhc9dsfwFJ0uD09E4kM+9oB0izLYFvtzZNND/Xtbbt\nngyQRjs01s5kopKk4dPrO5EniIilPD4wPtP8PL217YGuw9rrSyPipMzc3/W6m4BNAMuXL2d8fLw0\nv+VPgUvPOlQ6draqcz7eTExMeK6GnDUafoOuUSlEIuJE4Fbg5GbTbXQueQEsbnV9rOvQ7vUlwONC\nJDO3AFsA1qxZk2NjY5Up8t6bbmXzjnJGzsqui8YGMu6oGR8fp1pfzQ9rNPwGXaMZ/5WNiBXAF4Az\nm01bgVdn5uSH5gda3Rd1Hd69PoEkaWTN6MuGzYfmX+VIgNwMvCIzH2l1+06rfUrXS5zaaj/YfSlL\nkjRaeg6RiFgL3AmsaDZtBl6bmY92dd3aaq+MiPZ3Rc5ttbfNZKKSpOHT0+WsiLgQ+DjwC82mjwGf\nBs6OiMluBzNze2buiIhtdO6+CuCWiLiWzndJLm76Jp3vnEiSRlivn4lcwJEAAXhds7TtBlY37TfQ\n+cb6CuCFwKe6+l6dmXfOaKaSpKHTlwcwZuZ9wAuA64GddO7K2k/nUtcFmXllP8aVJM2vnt6JZOZG\nYONMXjgz9wJvaxZJ0gLko+AlSWWGiCSpzBCRJJUN5rkgC9zqyz43kHF3XXfeQMaVdPzynYgkqcwQ\nkSSVGSKSpDJDRJJUZohIksoMEUlSmSEiSSozRCRJZYaIJKnMEJEklRkikqQyQ0SSVGaISJLKDBFJ\nUpkhIkkqM0QkSWWGiCSpzBCRJJUZIpKkMkNEklRmiEiSygwRSVKZISJJKjNEJEllhogkqcwQkSSV\nGSKSpDJDRJJUdsKgJ6C5s/qyzw1s7F3XnTewsSUNju9EJEllhogkqcwQkSSVGSKSpDJDRJJUZohI\nksoMEUlSmSEiSSozRCRJZYaIJKnMEJEklfU9RCLilRFxe0Tsi4iDEXFvRGyOiKX9HluS1F99DZGI\nuAq4FVgPnAwsAn4FeDuwPSJO6+f4kqT+6luIRMQ5wDub1cPA5cCFwNeabauBv+3X+JKk/uvno+Av\nAaJpfygzrwWIiHuA3c2+l0bEczPzW32ch+ZB5TH0l551iI0DfHy9pteu0fH4uH//eYXp9fNy1lir\nfedkIzO/C+xp7fvtPs5BktRHfQmRiDgZ+KXWpge6urTXn9OPOUiS+q9fl7MWd60/doz1Je0dEbEJ\n2NSsTkTE/xTnsAz4UfFYzYM/sUZDr12j+IsBT+Y4M4PzPdV/R6vmdDLH0K8QOdC1vugY6xPtHZm5\nBdgy2wlExPbMXDPb11H/WKPhZ42G36Br1JfLWZn5EPBQa9MpXV1ObbV39mMOkqT+6+cH69ta7XMm\nGxHxbOC0o/STJI2Qft7i+x7gVU17Y0TsBP6LzvdFJn0pM/+zT+PP+pKY+s4aDT9rNPwGWqPIzP69\neMQ1PD402vYA52bm7r5NQJLUV30NEYCI2AD8MfAbwFOB7wKfAa7NzB/2dXBJUl/1PUQkSQvXgnsU\nvE8NnlsRcUlE3BIR90VEtpaNR+m/rDnf9zbnf19Tj/OPMcaMalYZY6GKiOdHxDUR8ZWI2BMRP42I\nAxHxzYh4V0QsmeIYazSPImJlRHwwIr4RET+IiJ9FxCMR8e2I+LuIeN4Ux4xOjTJzwSzAVUAeZbkP\nOG3Qcxy1Bdh/lPO5cYq+q+g8F+1oNXjnbGtWGWMhL8DfHONcJPAt4OnWaKA1GpumRgeBF49qjQZ+\nguewUOfQeVpwAj8H3gFsAL7aOjFfHPQ8R20BvgLcCLwF+EHrXG6cou+XWvu/1pz/y5p6ZFOfs2dT\ns5mOsdAXOiHyIHA9cAFwHnBz1x+EK6zRQGu0Bvgo8CbgFcBLgauBn7XO002jWqOBn+A5LNQnWyfl\ng63tp7VOcALPHfRcR3UBdrXO48aufWe29h0GVrT2fbC175ZqzSpjLPQFOBd4Wte2AL7ZOh+ft0bD\nt9D5t5Ymz8dnR7VGC+kzkbFW26cGz791rfbuzLy/tX5Xq7221R5rtXupWWWMBS0z78jMn3RtS+Db\nrU2TjxayRkMgIpZExMuAs1ubv9j8HLkaLYgQ8anBQ+H0VvtY539pRJxUrNmMxphmvgtW80Fq+w/F\nZ5qf1miAIuL6iEjgJ8BtwFI6D058F/D+ptvI1WhBhAizeGqw5ky7Bsc6/9CpQaVmMx3juBMRJ9K5\nTHJys+k2OtfjwRoNq0XAk5v2yNWon489mU/lpwZrzrRrcKzzD50axDR9pqrZTMc4rkTECuALdK55\nA2wFXp2Zh5t1azRYNwCfAE4CfhN4O53HuF8OPJPOB+8jV6MFESKZ+VBEPMSR//vyqcHz7zut9rHO\n/4OZuR+gULMZj3G8iIgz6QTIimbTzcDFmfloq5s1GqDMvI/O7bYA/xIR3wM+0Ky/PiLeygjWaKFc\nzgKfGjxoW1vtlRGxsrV+bqu97SjtXmpWGWPBi4i1dD5QnQyQzcBruwIErNFARMRTj7LrcKv9ZODp\njGKNBn2b2xzeLvdbHLk17ed03iJuAL7e2n77oOc5agude9o3NMve1rl8T2v7sqbv1tb+u5t9l3Pk\nNsPDwEtmU7OZjrHQF+BC4NHWOfko8JKuZU31/FmjOanR14F/Bv6IzvdEXg68E3i4dZ52jmqNBn6C\n57hY17ROTPeyG1g16DmO2sLjvxtytGWs6ftsOg/YPFq/K2dbs8oYC3kBPtxDfXbN5vxZo1nX6D+m\nqc8EsHZUazTwE9yHgm0AvkznX1Z8FPhf4K+AZwx6bqO4MIMQafo/E/jr5rw/2tThy8Ar56pmlTEW\n6jLTELFGA6nR6+m8E9lJ593HITqPE9oO/CWwci7O36Bq5FN8JUllC+mDdUnSPDNEJEllhogkqcwQ\nkSSVGSKSpDJDRJJUZohIksoMEUlSmSEiSSozRCRJZf8PYxcOLu61EJYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18a7b8d4ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"HS2\"] = df[\"HS10\"].str[:2]\n",
    "df[\"HS4\"] = df[\"HS10\"].str[:4]\n",
    "df[\"HS6\"] = df[\"HS10\"].str[:6]\n",
    "\n",
    "# we'll look at the distribution of # of records in each class at the 2-digit level.\n",
    "# Remember, we haven't de-duplicated, so this estimate is a bit high\n",
    "df.HS2.value_counts().hist()\n",
    "df.HS2.value_counts().describe().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A median category size of 160 is workable, but not huge. How useful this is depends on how consistent your text data is. In the case of these descriptions, it should be good enough. But in the case of real-world product descriptions, you generally want more per category, to capture the wide variety of language choice and industry jargon. In other applications, less text may be alright. Disaggregating the codes into 4-digit or beyond will make this worse. So, we'll stick with 2-digit.\n",
    "\n",
    "With that said, we see that some categories do have only 2 descriptions. Let's see how many there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 codes with only 2 descs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99    2\n",
       "Name: HS2, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcs = df.HS2.value_counts()\n",
    "print(len(vcs[vcs == 2]), \"codes with only 2 descs\")\n",
    "vcs[vcs == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 99 looks to be a very specific category, so we'd probably want to think more carfeully before classifying those. Going forward, we'll remove them.\n",
    "\n",
    "\n",
    "#### Training / test sets\n",
    "As a bare minimum, we need at least 2 records in any category we want to attempt to model. This is because we need to split our data into two pieces: the _training set_, which we'll develop the model on, and the _test set_, which we'll subsequently evaluate it on. We want to see how the model performs on descriptions it's never seen before. In practice, we almost certainly want more than 2. At least 3.\n",
    "\n",
    "In addition, validation sets and [_cross validation_](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6), which we may be able to get into today, is an important technique in machine learning that helps us avoid \"overfitting\" the model to the sample of data we're using and the particular split of training/test data that we create. It's easy to do in python with `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reusable components**\n",
    "\n",
    "As we begin to model and explore different techniques, we'll essentially repeat 3 steps:\n",
    "1. Develop features\n",
    "2. Train a model with those features\n",
    "3. Evaluate \n",
    "\n",
    "In theory, we could change any of these (evaluation metric, specific input features used, or the model)... but today, we'll focus on 1. and 2. . In order to make life easier, we'll set up a couple of reusable functions that we can call with different parameters. This will make exploring different modelling approaches easier, as we can stay organized and keep track of what we've done, what we've tried, etc. - while it's a small cost up front, I'd highly recommend doing this in your own workflow.\n",
    "\n",
    "Just below, I also wrote a function to split the training/test data (stratified), which would make it easy to try this approach with 4-digit, 6-digit, or larger codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initially, we have 28469 records\n",
      "after deduping, we have 22372 records\n",
      "after removing <3 record categories, there are 22370 records\n",
      "training set has 16777 records -- 74.99776486365668 percent -- and test set has 5593 records\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# we make these functions so we can easily modify or reuse them later\n",
    "def make_test_train(df, y_column):\n",
    "    # first, let's remove duplicates\n",
    "    print(\"initially, we have\", len(df), \"records\")\n",
    "    deduped = df.drop_duplicates(subset=[\"long_words_only\"])\n",
    "    print(\"after deduping, we have\", len(deduped), \"records\")\n",
    "\n",
    "    #now, let's remove any HS4 category with <2 records\n",
    "    vcs = deduped[y_column].value_counts()\n",
    "    to_include = vcs[vcs > 2].index\n",
    "    final_dataset = deduped[deduped[y_column].isin(to_include)]\n",
    "    print(\"after removing <3 record categories, there are\", len(final_dataset), \"records\")\n",
    "\n",
    "    # the stratify is important -- \n",
    "    # it's making sure that we have an instance of each category in both the train and test sets\n",
    "\n",
    "    train, test = train_test_split(final_dataset, stratify=final_dataset[y_column], \n",
    "                                   random_state=42, )\n",
    "    print(\"training set has\", len(train), \"records --\", 100 * len(train) / len(final_dataset), \n",
    "          \"percent -- and test set has\", len(test), \"records\")\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train, test = make_test_train(df, \"HS2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've split up our data, we can choose a classifier. We'll use one of the simplest out there: [Logistic Regression](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc), often known as \"logit.\" Normally, logistic regression is a binary classifier. In our case, because we're categorizing  98 chapters (we dropped chapter 99), we'll actually be training 98 binary classifiers, and selecting the highest-probability prediction from those. This is known as \"one-vs-all\". There are other voting schemes to convert binary classifiers into multi-class classifiers.\n",
    "\n",
    "One more trick: instead of using the typical solvers used for logistic regression, we'll use a heuristic optimization approach that runs more quickly and efficiently, called \"stochastic gradient descent.\" SGD, as it's abbreviated, is part of the core technique used to optimize (the ever-trendy) neural networks (back-propagation or \"backprop\") as well. We'll leave it to you to convince yourself that this approach is just as good as a more involved optimization approach. You can get into the math or just try it out in Python/scikit-learn (look into the `LogisticRegression` class)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# here's one of those reusable functions I mentioned above\n",
    "def make_model(train, vec, column_name, y_column, loss='log', alpha=.0001):\n",
    "    # first, turn our words into a large matrix of 0s and 1s as demonstrated above\n",
    "    X = vec.fit_transform(train[column_name])\n",
    "    # pull out the thing we want to predict\n",
    "    y = train[y_column]\n",
    "\n",
    "    # instantiate a classifier. Note the \n",
    "    # hyperparameters, such as \"penalty\" and \"loss.\"\n",
    "    # We'll come back to those.\n",
    "    clf = SGDClassifier(n_jobs=NCPUS, alpha=alpha, loss=loss,\n",
    "                        penalty='l2', max_iter=5000, tol=1e-5,\n",
    "                        random_state=42)\n",
    "    \n",
    "    # once we've specified what we want the classifier to look like\n",
    "    # we call its 'fit' function to actually train a model.\n",
    "    clf.fit(X, y)\n",
    "    return clf, vec\n",
    "\n",
    "clf, vec = make_model(train, CountVectorizer(), \"long_words_only\", \"HS2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained model in the `clf` variable. We also need to keep the \"trained\" vectorizer (i.e. the mapping of words to columns) in a variable, `vec`. Let's see how it does on a simple metric: overall accuracy. That is, \"out of every code the model predicted, what fraction did it get right?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-sample accuracy:  0.970137688502116\n",
      "test set accuracy:  0.9116753084212408\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(clf, vec, train, test, column_name, y_column):\n",
    "    # first, again extract the feature matrix for the training set\n",
    "    X = vec.transform(train[column_name])\n",
    "    y = train[y_column]\n",
    "    # now, extract feature matrix for test set.\n",
    "    # note that unknown words simply do not get mapped,\n",
    "    # which we'll see below\n",
    "    X_test = vec.transform(test[column_name])\n",
    "    \n",
    "    # this is how we actually use the model to \n",
    "    # predict outputs based on inputs\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    # and, we will compare to the actual labels in the test set\n",
    "    y_test_true = test[y_column]\n",
    "    \n",
    "    # last, we'll want to look at how well the model does on the training\n",
    "    # set as well, so let's predict those too.\n",
    "    y_train_pred = clf.predict(X)\n",
    "    print(\"in-sample accuracy: \", (y_train_pred == y).mean())\n",
    "    print(\"test set accuracy: \", (y_test_pred == y_test_true).mean())\n",
    "    return y_test_true, y_test_pred\n",
    "    \n",
    "y_test_true, y_test_pred = evaluate_model(clf, vec, train, test, \"long_words_only\", \"HS2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! \n",
    "\n",
    "It's worth noting that here we include the accuracy on the training set (\"in-sample\") and test set in order to diagnose overfitting. If the performance on the training set is dramatically better than the performance on the test set, it's often a sign that the model is learning \"the wrong things\" from the training data. \n",
    "\n",
    "This is an especially common problem in text processing, where we have a model that uses a large number of varialbes (1 per word, in the case of this initial model). This is sometimes called the \"curse of dimensionality.\" We'll look more at this and discuss a commonly used strategy later.\n",
    "\n",
    "Now, we can drill down a bit in a few ways. A productive way to do so is to look at poorly performing codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         01       0.92      1.00      0.96        22\n",
      "         02       0.96      0.98      0.97        53\n",
      "         03       0.95      0.98      0.96       158\n",
      "         04       0.92      0.96      0.94        74\n",
      "         05       1.00      0.42      0.59        12\n",
      "         06       1.00      0.87      0.93        15\n",
      "         07       0.97      1.00      0.98        85\n",
      "         08       0.92      0.96      0.94        57\n",
      "         09       0.95      0.82      0.88        22\n",
      "         10       0.94      0.84      0.89        19\n",
      "         11       0.76      0.93      0.84        14\n",
      "         12       0.81      0.90      0.85        42\n",
      "         13       1.00      0.14      0.25         7\n",
      "         14       1.00      0.75      0.86         4\n",
      "         15       0.96      0.87      0.91        30\n",
      "         16       0.91      0.77      0.83        56\n",
      "         17       0.96      0.96      0.96        26\n",
      "         18       0.92      0.96      0.94        23\n",
      "         19       0.95      0.70      0.81        30\n",
      "         20       0.90      0.95      0.93        79\n",
      "         21       1.00      0.68      0.81        34\n",
      "         22       1.00      0.85      0.92        33\n",
      "         23       0.93      0.82      0.87        17\n",
      "         24       1.00      0.98      0.99        57\n",
      "         25       0.74      0.59      0.65        29\n",
      "         26       0.91      0.97      0.94        30\n",
      "         27       0.90      0.70      0.79        40\n",
      "         28       0.92      0.67      0.77        90\n",
      "         29       0.67      0.97      0.80       330\n",
      "         30       0.86      0.64      0.73        28\n",
      "         31       1.00      0.50      0.67         8\n",
      "         32       0.97      0.79      0.87        47\n",
      "         33       1.00      0.71      0.83        21\n",
      "         34       0.90      0.64      0.75        14\n",
      "         35       1.00      0.44      0.62         9\n",
      "         36       1.00      0.40      0.57         5\n",
      "         37       0.94      0.89      0.91        18\n",
      "         38       0.74      0.66      0.70        56\n",
      "         39       0.85      0.76      0.80        89\n",
      "         40       0.93      0.91      0.92        58\n",
      "         41       0.98      1.00      0.99        51\n",
      "         42       1.00      0.89      0.94        35\n",
      "         43       1.00      1.00      1.00        11\n",
      "         44       0.98      0.97      0.98       142\n",
      "         45       1.00      0.83      0.91         6\n",
      "         46       1.00      1.00      1.00        14\n",
      "         47       1.00      0.57      0.73         7\n",
      "         48       0.96      0.98      0.97        91\n",
      "         49       0.89      0.57      0.70        14\n",
      "         50       1.00      0.89      0.94         9\n",
      "         51       0.97      0.97      0.97        37\n",
      "         52       0.97      1.00      0.98       157\n",
      "         53       1.00      0.88      0.94        17\n",
      "         54       0.96      1.00      0.98        81\n",
      "         55       0.99      1.00      1.00       118\n",
      "         56       1.00      0.96      0.98        25\n",
      "         57       1.00      0.96      0.98        23\n",
      "         58       0.94      0.97      0.96        34\n",
      "         59       0.91      0.91      0.91        22\n",
      "         60       1.00      1.00      1.00        33\n",
      "         61       1.00      0.99      0.99       248\n",
      "         62       0.98      1.00      0.99       346\n",
      "         63       0.95      0.91      0.93        69\n",
      "         64       0.98      1.00      0.99       109\n",
      "         65       1.00      1.00      1.00        18\n",
      "         66       0.00      0.00      0.00         2\n",
      "         67       1.00      0.75      0.86         4\n",
      "         68       0.75      0.72      0.74        29\n",
      "         69       0.97      0.78      0.86        36\n",
      "         70       0.92      0.97      0.94        68\n",
      "         71       0.90      0.88      0.89        41\n",
      "         72       1.00      0.94      0.97       165\n",
      "         73       0.93      0.99      0.96       169\n",
      "         74       0.91      0.91      0.91        44\n",
      "         75       0.80      0.89      0.84         9\n",
      "         76       0.92      1.00      0.96        36\n",
      "         78       0.67      0.67      0.67         3\n",
      "         79       1.00      1.00      1.00         4\n",
      "         80       1.00      0.25      0.40         4\n",
      "         81       0.83      0.95      0.88        20\n",
      "         82       0.84      0.88      0.86        59\n",
      "         83       0.95      0.76      0.84        25\n",
      "         84       0.90      0.94      0.92       461\n",
      "         85       0.87      0.91      0.89       269\n",
      "         86       1.00      0.67      0.80         9\n",
      "         87       0.93      0.93      0.93       107\n",
      "         88       1.00      0.87      0.93        15\n",
      "         89       0.86      0.55      0.67        11\n",
      "         90       0.86      0.82      0.84       128\n",
      "         91       1.00      0.99      1.00       105\n",
      "         92       1.00      0.68      0.81        19\n",
      "         93       0.86      0.60      0.71        20\n",
      "         94       0.87      0.91      0.89        66\n",
      "         95       0.95      0.55      0.69        33\n",
      "         96       0.75      0.72      0.73        46\n",
      "         97       0.00      0.00      0.00         5\n",
      "         98       0.89      0.74      0.81        23\n",
      "\n",
      "avg / total       0.92      0.91      0.91      5593\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "print(classification_report(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at codes with >= 100 samples can see chapter 29 is performing poorly. Let's investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_words_only</th>\n",
       "      <th>HS2</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>bauxite calcined refractory grade</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>toluene</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>quebracho extract</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>imitation gemstones</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>trisodium phosphate</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>nuclear reactors</td>\n",
       "      <td>84</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>flashlights</td>\n",
       "      <td>85</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>nitric acid and sulfonitric acids</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>peptones and their derivatives other protein substances and their derivatives and hide powder whether or not chromed nesoi</td>\n",
       "      <td>35</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>golf clubs complete</td>\n",
       "      <td>95</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>coffee husks and skins</td>\n",
       "      <td>09</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>paintballs</td>\n",
       "      <td>93</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>manostats</td>\n",
       "      <td>90</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>herrings kipper snacks</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>squash rackets</td>\n",
       "      <td>95</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>bis 1 2 2 6 6 pentamethyl 4 piperidinyl sebacate</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>cerium compounds</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>ambergris castoreum civet and musk</td>\n",
       "      <td>05</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>chrysotile crudes</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>metaldehyde powder or crystal form</td>\n",
       "      <td>29</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>inflatable rafts</td>\n",
       "      <td>89</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>other vitamins synthesized from aromatic or modified aromatic industrial organic compounds nesoi</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>heparin and its salts other human or animal substances prepared for therapeutic or prophylactic uses not elsewhere specified or included</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>carrageenan</td>\n",
       "      <td>13</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>stereoscopic microscopes</td>\n",
       "      <td>90</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>tomato ketchup</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>western red cedar shingles</td>\n",
       "      <td>44</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>crispbread</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>rangefinders</td>\n",
       "      <td>90</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>ferrotungsten and ferrosilicon tungsten</td>\n",
       "      <td>72</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>nitrites</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4534</th>\n",
       "      <td>benzene</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4646</th>\n",
       "      <td>mineral substances not elsewhere specified or included</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>polypropylene</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4716</th>\n",
       "      <td>sodium</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4766</th>\n",
       "      <td>multiline telephones including key call director consoles</td>\n",
       "      <td>85</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>snuff and snuff flour</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4806</th>\n",
       "      <td>polyamide 6 11 12 6 6 6 9 6 10 or 6 12</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4840</th>\n",
       "      <td>synthetic organic tanning substances aromatic or modified aromatic</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4859</th>\n",
       "      <td>other calcareous stone nesoi</td>\n",
       "      <td>68</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4888</th>\n",
       "      <td>disinfectants</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>polyphosphoric acids</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>combination antibiotics containing penicllins or derivatives thereof with a penicillanic acid structure or streptomycins or their derivatives nesoi</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5029</th>\n",
       "      <td>n vinyl 2 pyrrolidone monomer</td>\n",
       "      <td>29</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5087</th>\n",
       "      <td>ephedra</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>other including natural concentrates</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>ammonium dihydrogenorthophosphate monoammonium phosphate</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5211</th>\n",
       "      <td>n 2 2 6 dicyano 4 methylphenylazo 5 diethylamino phenyl methanesulfonamide and n 2 2 6 dicyano 4 methyl phenylazo 5 di 1 propylamino phenyl m</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5212</th>\n",
       "      <td>gum arabic</td>\n",
       "      <td>13</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5245</th>\n",
       "      <td>catalytic converters</td>\n",
       "      <td>84</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5258</th>\n",
       "      <td>malted milk</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5268</th>\n",
       "      <td>sulfur monochloride</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5271</th>\n",
       "      <td>pectic substances pectinates and pectates</td>\n",
       "      <td>13</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5333</th>\n",
       "      <td>4 4 isopropylidenedicyclohexanol and mixtures contg not less than 90 by weight of stereoisomers of 2 isopropyl 5 methylcyclohexanol</td>\n",
       "      <td>29</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5388</th>\n",
       "      <td>vermiculite perlite and chlorites unexpanded</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5389</th>\n",
       "      <td>contact lenses</td>\n",
       "      <td>90</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>racquetball rackets</td>\n",
       "      <td>95</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5464</th>\n",
       "      <td>diammonium hydrogenorthophosphate diammonium phosphate</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5471</th>\n",
       "      <td>carbonyl dichloride phosgene</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>tumeric curcuma</td>\n",
       "      <td>09</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          long_words_only  \\\n",
       "57    bauxite calcined refractory grade                                                                                                                     \n",
       "60    toluene                                                                                                                                               \n",
       "61    quebracho extract                                                                                                                                     \n",
       "126   imitation gemstones                                                                                                                                   \n",
       "169   trisodium phosphate                                                                                                                                   \n",
       "190   nuclear reactors                                                                                                                                      \n",
       "226   flashlights                                                                                                                                           \n",
       "229   nitric acid and sulfonitric acids                                                                                                                     \n",
       "243   peptones and their derivatives other protein substances and their derivatives and hide powder whether or not chromed nesoi                            \n",
       "302   golf clubs complete                                                                                                                                   \n",
       "305   coffee husks and skins                                                                                                                                \n",
       "307   paintballs                                                                                                                                            \n",
       "343   manostats                                                                                                                                             \n",
       "359   herrings kipper snacks                                                                                                                                \n",
       "364   squash rackets                                                                                                                                        \n",
       "365   bis 1 2 2 6 6 pentamethyl 4 piperidinyl sebacate                                                                                                      \n",
       "400   cerium compounds                                                                                                                                      \n",
       "432   ambergris castoreum civet and musk                                                                                                                    \n",
       "492   chrysotile crudes                                                                                                                                     \n",
       "565   metaldehyde powder or crystal form                                                                                                                    \n",
       "570   inflatable rafts                                                                                                                                      \n",
       "593   other vitamins synthesized from aromatic or modified aromatic industrial organic compounds nesoi                                                      \n",
       "672   heparin and its salts other human or animal substances prepared for therapeutic or prophylactic uses not elsewhere specified or included              \n",
       "741   carrageenan                                                                                                                                           \n",
       "756   stereoscopic microscopes                                                                                                                              \n",
       "766   tomato ketchup                                                                                                                                        \n",
       "801   western red cedar shingles                                                                                                                            \n",
       "948   crispbread                                                                                                                                            \n",
       "971   rangefinders                                                                                                                                          \n",
       "983   ferrotungsten and ferrosilicon tungsten                                                                                                               \n",
       "...                                       ...                                                                                                               \n",
       "4500  nitrites                                                                                                                                              \n",
       "4534  benzene                                                                                                                                               \n",
       "4646  mineral substances not elsewhere specified or included                                                                                                \n",
       "4667  polypropylene                                                                                                                                         \n",
       "4716  sodium                                                                                                                                                \n",
       "4766  multiline telephones including key call director consoles                                                                                             \n",
       "4799  snuff and snuff flour                                                                                                                                 \n",
       "4806  polyamide 6 11 12 6 6 6 9 6 10 or 6 12                                                                                                                \n",
       "4840  synthetic organic tanning substances aromatic or modified aromatic                                                                                    \n",
       "4859  other calcareous stone nesoi                                                                                                                          \n",
       "4888  disinfectants                                                                                                                                         \n",
       "4982  polyphosphoric acids                                                                                                                                  \n",
       "5026  combination antibiotics containing penicllins or derivatives thereof with a penicillanic acid structure or streptomycins or their derivatives nesoi   \n",
       "5029  n vinyl 2 pyrrolidone monomer                                                                                                                         \n",
       "5087  ephedra                                                                                                                                               \n",
       "5170  other including natural concentrates                                                                                                                  \n",
       "5206  ammonium dihydrogenorthophosphate monoammonium phosphate                                                                                              \n",
       "5211  n 2 2 6 dicyano 4 methylphenylazo 5 diethylamino phenyl methanesulfonamide and n 2 2 6 dicyano 4 methyl phenylazo 5 di 1 propylamino phenyl m         \n",
       "5212  gum arabic                                                                                                                                            \n",
       "5245  catalytic converters                                                                                                                                  \n",
       "5258  malted milk                                                                                                                                           \n",
       "5268  sulfur monochloride                                                                                                                                   \n",
       "5271  pectic substances pectinates and pectates                                                                                                             \n",
       "5333  4 4 isopropylidenedicyclohexanol and mixtures contg not less than 90 by weight of stereoisomers of 2 isopropyl 5 methylcyclohexanol                   \n",
       "5388  vermiculite perlite and chlorites unexpanded                                                                                                          \n",
       "5389  contact lenses                                                                                                                                        \n",
       "5425  racquetball rackets                                                                                                                                   \n",
       "5464  diammonium hydrogenorthophosphate diammonium phosphate                                                                                                \n",
       "5471  carbonyl dichloride phosgene                                                                                                                          \n",
       "5523  tumeric curcuma                                                                                                                                       \n",
       "\n",
       "     HS2 pred  \n",
       "57    26  29   \n",
       "60    27  29   \n",
       "61    32  29   \n",
       "126   39  29   \n",
       "169   28  29   \n",
       "190   84  29   \n",
       "226   85  29   \n",
       "229   28  29   \n",
       "243   35  29   \n",
       "302   95  29   \n",
       "305   09  29   \n",
       "307   93  29   \n",
       "343   90  29   \n",
       "359   16  29   \n",
       "364   95  29   \n",
       "365   38  29   \n",
       "400   28  29   \n",
       "432   05  29   \n",
       "492   25  29   \n",
       "565   29  71   \n",
       "570   89  29   \n",
       "593   30  29   \n",
       "672   30  29   \n",
       "741   13  29   \n",
       "756   90  29   \n",
       "766   21  29   \n",
       "801   44  29   \n",
       "948   19  29   \n",
       "971   90  29   \n",
       "983   72  29   \n",
       "...   ..  ..   \n",
       "4500  28  29   \n",
       "4534  27  29   \n",
       "4646  25  29   \n",
       "4667  39  29   \n",
       "4716  28  29   \n",
       "4766  85  29   \n",
       "4799  24  29   \n",
       "4806  39  29   \n",
       "4840  32  29   \n",
       "4859  68  29   \n",
       "4888  38  29   \n",
       "4982  28  29   \n",
       "5026  30  29   \n",
       "5029  29  39   \n",
       "5087  12  29   \n",
       "5170  29  25   \n",
       "5206  31  29   \n",
       "5211  32  29   \n",
       "5212  13  29   \n",
       "5245  84  29   \n",
       "5258  19  29   \n",
       "5268  28  29   \n",
       "5271  13  29   \n",
       "5333  29  38   \n",
       "5388  25  29   \n",
       "5389  90  29   \n",
       "5425  95  29   \n",
       "5464  31  29   \n",
       "5471  28  29   \n",
       "5523  09  29   \n",
       "\n",
       "[165 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reset = test.reset_index() # this is an unfortunate pandas thing \n",
    "test_reset[\"pred\"] = y_test_pred # add a column that is our predicted values from above\n",
    "\n",
    "# this is the syntax for queries in pandas\n",
    "# | is for the boolean \"OR\", == is roughly \"WHERE x=y\", and != is \"WHERE x!=y\"\n",
    "CODE = \"29\"\n",
    "cond1 = (test_reset[\"HS2\"] == CODE) | (test_reset[\"pred\"] == CODE)\n",
    "cond2 = (test_reset[\"HS2\"] != test_reset[\"pred\"])\n",
    "\n",
    "cols = [\"long_words_only\", \"HS2\", \"pred\"] # just specifying the columns to view\n",
    "test_reset[cond1 & cond2][cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here? There doesn't appear to be much of a pattern here. It's likely that 29 is the \"default\" prediction. This makes sense, because the category is both large and full of obscure chemical words. The model can likely figure out other chapters more easily, so it's a smart strategy to, upon seeing an unknown word, pick the category with the largest variety of complex words. We can test this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['29'], dtype='<U2')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's get the vector for a word the model has never seen before -- it's all zeroes\n",
    "x = vec.transform([\"thiswordisgibberish\"])\n",
    "# then, we'll run it through the model\n",
    "clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalently, we can see which of the intercept terms is largest.\n",
    "# this will always be the default prediction in the absence of known words.\n",
    "clf.classes_[clf.intercept_.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems this intuition is correct.\n",
    "\n",
    "One interesting result is that \"racquetball rackets\" gets predicted to code 29. What's going on there? Let's check out the training set to see how frequently the words \"racquetball\" and \"racket\" occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS10</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>long_words_only</th>\n",
       "      <th>HS2</th>\n",
       "      <th>HS4</th>\n",
       "      <th>HS6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17348</th>\n",
       "      <td>8708806510</td>\n",
       "      <td>BEAM HANGER BRACKETS FOR SUSPENSION SYSTEMS, NESOI, OF THE MOTOR VEHICLES OF HEADING 8701 - 8705</td>\n",
       "      <td>beam hanger brackets for suspension systems nesoi of the motor vehicles of heading 8701 8705</td>\n",
       "      <td>87</td>\n",
       "      <td>8708</td>\n",
       "      <td>870880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18793</th>\n",
       "      <td>9506516000</td>\n",
       "      <td>PARTS AND ACCESSORIES FOR LAWN-TENNIS RACKETS</td>\n",
       "      <td>parts and accessories for lawn tennis rackets</td>\n",
       "      <td>95</td>\n",
       "      <td>9506</td>\n",
       "      <td>950651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18791</th>\n",
       "      <td>9506512000</td>\n",
       "      <td>LAWN-TENNIS RACKETS, STRUNG</td>\n",
       "      <td>lawn tennis rackets strung</td>\n",
       "      <td>95</td>\n",
       "      <td>9506</td>\n",
       "      <td>950651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18792</th>\n",
       "      <td>9506514000</td>\n",
       "      <td>LAWN-TENNIS RACKETS, NOT STRUNG</td>\n",
       "      <td>lawn tennis rackets not strung</td>\n",
       "      <td>95</td>\n",
       "      <td>9506</td>\n",
       "      <td>950651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14524</th>\n",
       "      <td>8302500000</td>\n",
       "      <td>HAT-RACKS, HAT PEGS, BRACKETS AND SIMILAR FIXTURES, AND PARTS THEREOF, OF BASE METAL</td>\n",
       "      <td>hat racks hat pegs brackets and similar fixtures and parts thereof of base metal</td>\n",
       "      <td>83</td>\n",
       "      <td>8302</td>\n",
       "      <td>830250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18798</th>\n",
       "      <td>9506598060</td>\n",
       "      <td>RACKETS WHETHER OR NOT STRUNG, INCLUDING PARTS AND ACCESSORIES, NESOI</td>\n",
       "      <td>rackets whether or not strung including parts and accessories nesoi</td>\n",
       "      <td>95</td>\n",
       "      <td>9506</td>\n",
       "      <td>950659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28360</th>\n",
       "      <td>9506511000</td>\n",
       "      <td>LAWN-TENNIS RACKETS, WHETHER OR NOT STRUNG</td>\n",
       "      <td>lawn tennis rackets whether or not strung</td>\n",
       "      <td>95</td>\n",
       "      <td>9506</td>\n",
       "      <td>950651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18728</th>\n",
       "      <td>9405916040</td>\n",
       "      <td>LAMPS PARTS, PRISMS AND OTHER GLASS ARTICLES OF A KIND USE IN CHANDELIERS AND WALL BRACKETS, AND ARTICLES THEREOF</td>\n",
       "      <td>lamps parts prisms and other glass articles of a kind use in chandeliers and wall brackets and articles thereof</td>\n",
       "      <td>94</td>\n",
       "      <td>9405</td>\n",
       "      <td>940591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18794</th>\n",
       "      <td>9506594040</td>\n",
       "      <td>BADMINTON RACKETS AND RACKET FRAMES</td>\n",
       "      <td>badminton rackets and racket frames</td>\n",
       "      <td>95</td>\n",
       "      <td>9506</td>\n",
       "      <td>950659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18822</th>\n",
       "      <td>9506991200</td>\n",
       "      <td>BADMINTON ARTICLES AND EQUIPMENT EXCEPT RACKETS AND PARTS AND ACCESSORIES THEREOF, NESOI</td>\n",
       "      <td>badminton articles and equipment except rackets and parts and accessories thereof nesoi</td>\n",
       "      <td>95</td>\n",
       "      <td>9506</td>\n",
       "      <td>950699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             HS10  \\\n",
       "17348  8708806510   \n",
       "18793  9506516000   \n",
       "18791  9506512000   \n",
       "18792  9506514000   \n",
       "14524  8302500000   \n",
       "18798  9506598060   \n",
       "28360  9506511000   \n",
       "18728  9405916040   \n",
       "18794  9506594040   \n",
       "18822  9506991200   \n",
       "\n",
       "                                                                                                                                                    long_desc  \\\n",
       "17348  BEAM HANGER BRACKETS FOR SUSPENSION SYSTEMS, NESOI, OF THE MOTOR VEHICLES OF HEADING 8701 - 8705                                                         \n",
       "18793  PARTS AND ACCESSORIES FOR LAWN-TENNIS RACKETS                                                                                                            \n",
       "18791  LAWN-TENNIS RACKETS, STRUNG                                                                                                                              \n",
       "18792  LAWN-TENNIS RACKETS, NOT STRUNG                                                                                                                          \n",
       "14524  HAT-RACKS, HAT PEGS, BRACKETS AND SIMILAR FIXTURES, AND PARTS THEREOF, OF BASE METAL                                                                     \n",
       "18798  RACKETS WHETHER OR NOT STRUNG, INCLUDING PARTS AND ACCESSORIES, NESOI                                                                                    \n",
       "28360  LAWN-TENNIS RACKETS, WHETHER OR NOT STRUNG                                                                                                               \n",
       "18728  LAMPS PARTS, PRISMS AND OTHER GLASS ARTICLES OF A KIND USE IN CHANDELIERS AND WALL BRACKETS, AND ARTICLES THEREOF                                        \n",
       "18794  BADMINTON RACKETS AND RACKET FRAMES                                                                                                                      \n",
       "18822  BADMINTON ARTICLES AND EQUIPMENT EXCEPT RACKETS AND PARTS AND ACCESSORIES THEREOF, NESOI                                                                 \n",
       "\n",
       "                                                                                                       long_words_only  \\\n",
       "17348  beam hanger brackets for suspension systems nesoi of the motor vehicles of heading 8701 8705                      \n",
       "18793  parts and accessories for lawn tennis rackets                                                                     \n",
       "18791  lawn tennis rackets strung                                                                                        \n",
       "18792  lawn tennis rackets not strung                                                                                    \n",
       "14524  hat racks hat pegs brackets and similar fixtures and parts thereof of base metal                                  \n",
       "18798  rackets whether or not strung including parts and accessories nesoi                                               \n",
       "28360  lawn tennis rackets whether or not strung                                                                         \n",
       "18728  lamps parts prisms and other glass articles of a kind use in chandeliers and wall brackets and articles thereof   \n",
       "18794  badminton rackets and racket frames                                                                               \n",
       "18822  badminton articles and equipment except rackets and parts and accessories thereof nesoi                           \n",
       "\n",
       "      HS2   HS4     HS6  \n",
       "17348  87  8708  870880  \n",
       "18793  95  9506  950651  \n",
       "18791  95  9506  950651  \n",
       "18792  95  9506  950651  \n",
       "14524  83  8302  830250  \n",
       "18798  95  9506  950659  \n",
       "28360  95  9506  950651  \n",
       "18728  94  9405  940591  \n",
       "18794  95  9506  950659  \n",
       "18822  95  9506  950699  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train[\"long_words_only\"].str.contains(\"racket\")\n",
    "      | train[\"long_words_only\"].str.contains(\"racquetball\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Racquetball\" isn't there, but \"racket\" is, and we'd hope that would be enough information (it certainly is for a human). It may be the case that some of these other words are polluting the model's ability to grasp the importance of the word racket. Let's remove some of them using a common NLP technique. The idea is that we want to remove some of the most commonly occurring words immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "all_stopwords = set(stopwords.words('english'))\n",
    "all_stopwords.add(\"nesoi\")\n",
    "all_stopwords.add(\"excluding\")\n",
    "all_stopwords.add(\"except\")\n",
    "all_stopwords.add(\"including\")\n",
    "all_stopwords.add(\"thereof\")\n",
    "all_stopwords.add(\"parts\")\n",
    "all_stopwords.add(\"accessories\")\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(desc):\n",
    "    return \" \".join(d for d in desc.split() if d not in all_stopwords)\n",
    "df[\"long_no_stopwords\"] = df[\"long_words_only\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initially, we have 28469 records\n",
      "after deduping, we have 22372 records\n",
      "after removing <3 record categories, there are 22370 records\n",
      "training set has 16777 records -- 74.99776486365668 percent -- and test set has 5593 records\n",
      "in-sample accuracy:  0.9597067413721165\n",
      "test set accuracy:  0.9050598962989451\n"
     ]
    }
   ],
   "source": [
    "train, test = make_test_train(df, \"HS2\")\n",
    "# because of how we coded the function above, we just specify that we want to use\n",
    "# a different column name for the input text\n",
    "clf, vec = make_model(train, CountVectorizer(binary=True), \"long_no_stopwords\", \"HS2\")\n",
    "y_test_true, y_test_pred = evaluate_model(clf, vec, train, test, \"long_no_stopwords\", \"HS2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps marginally worse performance, but with a decrease this small, this could simply be a function of the random train/test split we chose. Without further investigation, I'd say this had no improvement either way on this dataset. As compared with other text-based datasets, this may be a result of the fact that these descriptions are more standardized and direct than typical text data, so there aren't really a ton of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_no_stopwords</th>\n",
       "      <th>HS2</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>squash rackets</td>\n",
       "      <td>95</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>racquetball rackets</td>\n",
       "      <td>95</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        long_no_stopwords HS2 pred\n",
       "364   squash rackets       95  29 \n",
       "5425  racquetball rackets  95  29 "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reset = test.reset_index()\n",
    "test_reset[\"pred\"] = y_test_pred\n",
    "\n",
    "cols = [\"long_no_stopwords\", \"HS2\", \"pred\"]\n",
    "CODE = \"29\"\n",
    "cond1 = (test_reset[\"HS2\"] == CODE) | (test_reset[\"pred\"] == CODE)\n",
    "cond2 = (test_reset[\"HS2\"] != test_reset[\"pred\"])\n",
    "cond3 = (test_reset[cols[0]].str.contains(\"racket\"))\n",
    "test_reset[cond1 & cond2 & cond3][cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, we still see that the \"racquetball rackets\" description is mispredicted in the exact same way. \n",
    "\n",
    "Something else we can try is \"stemming\" -- the idea is, we want to exploit the fact that english words that convey the same meaning can have different suffixes. There are a number of approaches to stemming. For example, maybe if we turn all instances of \"rackets\" to the standardized \"racket\", we'll see some improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initially, we have 28469 records\n",
      "after deduping, we have 22372 records\n",
      "after removing <3 record categories, there are 22370 records\n",
      "training set has 16777 records -- 74.99776486365668 percent -- and test set has 5593 records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS10</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>long_words_only</th>\n",
       "      <th>HS2</th>\n",
       "      <th>HS4</th>\n",
       "      <th>HS6</th>\n",
       "      <th>long_no_stopwords</th>\n",
       "      <th>long_no_stopwords_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17348</th>\n",
       "      <td>8708806510</td>\n",
       "      <td>BEAM HANGER BRACKETS FOR SUSPENSION SYSTEMS, NESOI, OF THE MOTOR VEHICLES OF HEADING 8701 - 8705</td>\n",
       "      <td>beam hanger brackets for suspension systems nesoi of the motor vehicles of heading 8701 8705</td>\n",
       "      <td>87</td>\n",
       "      <td>8708</td>\n",
       "      <td>870880</td>\n",
       "      <td>beam hanger brackets suspension systems motor vehicles heading 8701 8705</td>\n",
       "      <td>beam hanger bracket suspens system motor vehicl head 8701 8705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18793</th>\n",
       "      <td>9506516000</td>\n",
       "      <td>PARTS AND ACCESSORIES FOR LAWN-TENNIS RACKETS</td>\n",
       "      <td>parts and accessories for lawn tennis rackets</td>\n",
       "      <td>95</td>\n",
       "      <td>9506</td>\n",
       "      <td>950651</td>\n",
       "      <td>lawn tennis rackets</td>\n",
       "      <td>lawn tenni racket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18791</th>\n",
       "      <td>9506512000</td>\n",
       "      <td>LAWN-TENNIS RACKETS, STRUNG</td>\n",
       "      <td>lawn tennis rackets strung</td>\n",
       "      <td>95</td>\n",
       "      <td>9506</td>\n",
       "      <td>950651</td>\n",
       "      <td>lawn tennis rackets strung</td>\n",
       "      <td>lawn tenni racket strung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18792</th>\n",
       "      <td>9506514000</td>\n",
       "      <td>LAWN-TENNIS RACKETS, NOT STRUNG</td>\n",
       "      <td>lawn tennis rackets not strung</td>\n",
       "      <td>95</td>\n",
       "      <td>9506</td>\n",
       "      <td>950651</td>\n",
       "      <td>lawn tennis rackets strung</td>\n",
       "      <td>lawn tenni racket strung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14524</th>\n",
       "      <td>8302500000</td>\n",
       "      <td>HAT-RACKS, HAT PEGS, BRACKETS AND SIMILAR FIXTURES, AND PARTS THEREOF, OF BASE METAL</td>\n",
       "      <td>hat racks hat pegs brackets and similar fixtures and parts thereof of base metal</td>\n",
       "      <td>83</td>\n",
       "      <td>8302</td>\n",
       "      <td>830250</td>\n",
       "      <td>hat racks hat pegs brackets similar fixtures base metal</td>\n",
       "      <td>hat rack hat peg bracket similar fixtur base metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18798</th>\n",
       "      <td>9506598060</td>\n",
       "      <td>RACKETS WHETHER OR NOT STRUNG, INCLUDING PARTS AND ACCESSORIES, NESOI</td>\n",
       "      <td>rackets whether or not strung including parts and accessories nesoi</td>\n",
       "      <td>95</td>\n",
       "      <td>9506</td>\n",
       "      <td>950659</td>\n",
       "      <td>rackets whether strung</td>\n",
       "      <td>racket whether strung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28360</th>\n",
       "      <td>9506511000</td>\n",
       "      <td>LAWN-TENNIS RACKETS, WHETHER OR NOT STRUNG</td>\n",
       "      <td>lawn tennis rackets whether or not strung</td>\n",
       "      <td>95</td>\n",
       "      <td>9506</td>\n",
       "      <td>950651</td>\n",
       "      <td>lawn tennis rackets whether strung</td>\n",
       "      <td>lawn tenni racket whether strung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18728</th>\n",
       "      <td>9405916040</td>\n",
       "      <td>LAMPS PARTS, PRISMS AND OTHER GLASS ARTICLES OF A KIND USE IN CHANDELIERS AND WALL BRACKETS, AND ARTICLES THEREOF</td>\n",
       "      <td>lamps parts prisms and other glass articles of a kind use in chandeliers and wall brackets and articles thereof</td>\n",
       "      <td>94</td>\n",
       "      <td>9405</td>\n",
       "      <td>940591</td>\n",
       "      <td>lamps prisms glass articles kind use chandeliers wall brackets articles</td>\n",
       "      <td>lamp prism glass articl kind use chandeli wall bracket articl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18794</th>\n",
       "      <td>9506594040</td>\n",
       "      <td>BADMINTON RACKETS AND RACKET FRAMES</td>\n",
       "      <td>badminton rackets and racket frames</td>\n",
       "      <td>95</td>\n",
       "      <td>9506</td>\n",
       "      <td>950659</td>\n",
       "      <td>badminton rackets racket frames</td>\n",
       "      <td>badminton racket racket frame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18822</th>\n",
       "      <td>9506991200</td>\n",
       "      <td>BADMINTON ARTICLES AND EQUIPMENT EXCEPT RACKETS AND PARTS AND ACCESSORIES THEREOF, NESOI</td>\n",
       "      <td>badminton articles and equipment except rackets and parts and accessories thereof nesoi</td>\n",
       "      <td>95</td>\n",
       "      <td>9506</td>\n",
       "      <td>950699</td>\n",
       "      <td>badminton articles equipment rackets</td>\n",
       "      <td>badminton articl equip racket</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             HS10  \\\n",
       "17348  8708806510   \n",
       "18793  9506516000   \n",
       "18791  9506512000   \n",
       "18792  9506514000   \n",
       "14524  8302500000   \n",
       "18798  9506598060   \n",
       "28360  9506511000   \n",
       "18728  9405916040   \n",
       "18794  9506594040   \n",
       "18822  9506991200   \n",
       "\n",
       "                                                                                                                                                    long_desc  \\\n",
       "17348  BEAM HANGER BRACKETS FOR SUSPENSION SYSTEMS, NESOI, OF THE MOTOR VEHICLES OF HEADING 8701 - 8705                                                         \n",
       "18793  PARTS AND ACCESSORIES FOR LAWN-TENNIS RACKETS                                                                                                            \n",
       "18791  LAWN-TENNIS RACKETS, STRUNG                                                                                                                              \n",
       "18792  LAWN-TENNIS RACKETS, NOT STRUNG                                                                                                                          \n",
       "14524  HAT-RACKS, HAT PEGS, BRACKETS AND SIMILAR FIXTURES, AND PARTS THEREOF, OF BASE METAL                                                                     \n",
       "18798  RACKETS WHETHER OR NOT STRUNG, INCLUDING PARTS AND ACCESSORIES, NESOI                                                                                    \n",
       "28360  LAWN-TENNIS RACKETS, WHETHER OR NOT STRUNG                                                                                                               \n",
       "18728  LAMPS PARTS, PRISMS AND OTHER GLASS ARTICLES OF A KIND USE IN CHANDELIERS AND WALL BRACKETS, AND ARTICLES THEREOF                                        \n",
       "18794  BADMINTON RACKETS AND RACKET FRAMES                                                                                                                      \n",
       "18822  BADMINTON ARTICLES AND EQUIPMENT EXCEPT RACKETS AND PARTS AND ACCESSORIES THEREOF, NESOI                                                                 \n",
       "\n",
       "                                                                                                       long_words_only  \\\n",
       "17348  beam hanger brackets for suspension systems nesoi of the motor vehicles of heading 8701 8705                      \n",
       "18793  parts and accessories for lawn tennis rackets                                                                     \n",
       "18791  lawn tennis rackets strung                                                                                        \n",
       "18792  lawn tennis rackets not strung                                                                                    \n",
       "14524  hat racks hat pegs brackets and similar fixtures and parts thereof of base metal                                  \n",
       "18798  rackets whether or not strung including parts and accessories nesoi                                               \n",
       "28360  lawn tennis rackets whether or not strung                                                                         \n",
       "18728  lamps parts prisms and other glass articles of a kind use in chandeliers and wall brackets and articles thereof   \n",
       "18794  badminton rackets and racket frames                                                                               \n",
       "18822  badminton articles and equipment except rackets and parts and accessories thereof nesoi                           \n",
       "\n",
       "      HS2   HS4     HS6  \\\n",
       "17348  87  8708  870880   \n",
       "18793  95  9506  950651   \n",
       "18791  95  9506  950651   \n",
       "18792  95  9506  950651   \n",
       "14524  83  8302  830250   \n",
       "18798  95  9506  950659   \n",
       "28360  95  9506  950651   \n",
       "18728  94  9405  940591   \n",
       "18794  95  9506  950659   \n",
       "18822  95  9506  950699   \n",
       "\n",
       "                                                              long_no_stopwords  \\\n",
       "17348  beam hanger brackets suspension systems motor vehicles heading 8701 8705   \n",
       "18793  lawn tennis rackets                                                        \n",
       "18791  lawn tennis rackets strung                                                 \n",
       "18792  lawn tennis rackets strung                                                 \n",
       "14524  hat racks hat pegs brackets similar fixtures base metal                    \n",
       "18798  rackets whether strung                                                     \n",
       "28360  lawn tennis rackets whether strung                                         \n",
       "18728  lamps prisms glass articles kind use chandeliers wall brackets articles    \n",
       "18794  badminton rackets racket frames                                            \n",
       "18822  badminton articles equipment rackets                                       \n",
       "\n",
       "                                            long_no_stopwords_stemmed  \n",
       "17348  beam hanger bracket suspens system motor vehicl head 8701 8705  \n",
       "18793  lawn tenni racket                                               \n",
       "18791  lawn tenni racket strung                                        \n",
       "18792  lawn tenni racket strung                                        \n",
       "14524  hat rack hat peg bracket similar fixtur base metal              \n",
       "18798  racket whether strung                                           \n",
       "28360  lawn tenni racket whether strung                                \n",
       "18728  lamp prism glass articl kind use chandeli wall bracket articl   \n",
       "18794  badminton racket racket frame                                   \n",
       "18822  badminton articl equip racket                                   "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "def stem_words(desc):\n",
    "    return \" \".join(stemmer.stem(x) for x in desc.split())\n",
    "\n",
    "df[\"long_no_stopwords_stemmed\"] = df[\"long_no_stopwords\"].apply(stem_words)\n",
    "train, test = make_test_train(df, \"HS2\")\n",
    "train[train[\"long_no_stopwords_stemmed\"].str.contains(\"racket\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-sample accuracy:  0.9573225248852596\n",
      "test set accuracy:  0.9057750759878419\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_no_stopwords</th>\n",
       "      <th>HS2</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>squash rackets</td>\n",
       "      <td>95</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>racquetball rackets</td>\n",
       "      <td>95</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        long_no_stopwords HS2 pred\n",
       "364   squash rackets       95  29 \n",
       "5425  racquetball rackets  95  29 "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf, vec = make_model(train, CountVectorizer(), \"long_no_stopwords_stemmed\", \"HS2\")\n",
    "y_test_true, y_test_pred = evaluate_model(clf, vec, train, test, \"long_no_stopwords_stemmed\", \"HS2\")\n",
    "test_reset = test.reset_index()\n",
    "test_reset[\"pred\"] = y_test_pred\n",
    "\n",
    "cols = [\"long_no_stopwords\", \"HS2\", \"pred\"]\n",
    "CODE = \"29\"\n",
    "cond1 = (test_reset[\"HS2\"] == CODE) | (test_reset[\"pred\"] == CODE)\n",
    "cond2 = (test_reset[\"HS2\"] != test_reset[\"pred\"])\n",
    "cond3 = (test_reset[cols[0]].str.contains(\"racket\"))\n",
    "test_reset[cond1 & cond2 & cond3][cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, no improvement. It's time to try some more involved methods.\n",
    "\n",
    "Let's try something that frequently helps bag-of-words models: we'll include _bigrams_ instead of unigrams. Frequently, in the English language, pairs of words -- and their order -- have meaning. While it may not help in a situation like the \"racquetball racket\" issue, it may help in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initially, we have 28469 records\n",
      "after deduping, we have 22372 records\n",
      "after removing <3 record categories, there are 22370 records\n",
      "training set has 16777 records -- 74.99776486365668 percent -- and test set has 5593 records\n",
      "in-sample accuracy:  0.9871848363831436\n",
      "test set accuracy:  0.9234757732880386\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_no_stopwords_stemmed</th>\n",
       "      <th>HS2</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>squash racket</td>\n",
       "      <td>95</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>racquetbal racket</td>\n",
       "      <td>95</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     long_no_stopwords_stemmed HS2 pred\n",
       "364   squash racket             95  29 \n",
       "5425  racquetbal racket         95  29 "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = make_test_train(df, \"HS2\")\n",
    "clf, vec = make_model(train, CountVectorizer(ngram_range=(1, 2)), \"long_words_only\", \"HS2\")\n",
    "y_test_true, y_test_pred = evaluate_model(clf, vec, train, test, \"long_words_only\", \"HS2\")\n",
    "\n",
    "test_reset = test.reset_index()\n",
    "test_reset[\"pred\"] = y_test_pred\n",
    "\n",
    "cols = [\"long_no_stopwords_stemmed\", \"HS2\", \"pred\"]\n",
    "CODE = \"29\"\n",
    "cond1 = (test_reset[\"HS2\"] == CODE) | (test_reset[\"pred\"] == CODE)\n",
    "cond2 = (test_reset[\"HS2\"] != test_reset[\"pred\"])\n",
    "cond3 = (test_reset[cols[0]].str.contains(\"racket\"))\n",
    "test_reset[cond1 & cond2 & cond3][cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've bumped our performance up about a percentage from our previous best, so this is potentially worth keeping around. However, in addition, training set accuracy is up to nearly 99%. This may indicate we're overfitting. The reason? When we include both words and all bigram combinations, we now have significantly more variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x54462 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 37 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.transform(train[:1]['long_words_only'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, this still doesn't solve our issue of \"racquetball racket.\" What's going on there? Something else that frequently helps models like these is _subword information_. Now, from each word, we're going to create features for all subwords of ranges 2-6 of that word. As an example, the subwords of length 3 for \"racquetball\" would be\n",
    "- rac\n",
    "- acq\n",
    "- que\n",
    "- uet\n",
    "- etb\n",
    "- bal\n",
    "- all (where a special character differentiates the word \"all\" from the character sequence a-l-l)\n",
    "\n",
    "Perhaps \"rac\" and \"bal\" will be enough to cue the model that these records should be in chapter 95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initially, we have 28469 records\n",
      "after deduping, we have 22372 records\n",
      "after removing <3 record categories, there are 22370 records\n",
      "training set has 16777 records -- 74.99776486365668 percent -- and test set has 5593 records\n",
      "in-sample accuracy:  0.9994635512904572\n",
      "test set accuracy:  0.9333094940103701\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_no_stopwords_stemmed</th>\n",
       "      <th>HS2</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [long_no_stopwords_stemmed, HS2, pred]\n",
       "Index: []"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "# without getting into the specifics of sklearn too much...\n",
    "# a FeatureUnion allows us to engineer separate features then throw\n",
    "# them all into a model. In this case, we'll continue using each word as \n",
    "# a feature, and also add in subwords of length 2-6\n",
    "fu = FeatureUnion([('word_counts', CountVectorizer()), \n",
    "                   ('char_counts', CountVectorizer(analyzer='char', ngram_range=(2, 6)))], n_jobs=2)\n",
    "\n",
    "train, test = make_test_train(df,  \"HS2\")\n",
    "clf, vec = make_model(train, fu, \"long_words_only\", \"HS2\")\n",
    "y_test_true, y_test_pred = evaluate_model(clf, vec, train, test, \"long_words_only\", \"HS2\")\n",
    "\n",
    "test_reset = test.reset_index()\n",
    "test_reset[\"pred\"] = y_test_pred\n",
    "\n",
    "cols = [\"long_no_stopwords_stemmed\", \"HS2\", \"pred\"]\n",
    "CODE = \"29\"\n",
    "cond1 = (test_reset[\"HS2\"] == CODE) | (test_reset[\"pred\"] == CODE)\n",
    "cond2 = (test_reset[\"HS2\"] != test_reset[\"pred\"])\n",
    "cond3 = (test_reset[cols[0]].str.contains(\"racket\"))\n",
    "test_reset[cond1 & cond2 & cond3][cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! We've fixed that particular issue. However, again, we see that we are currently 100% accurate. This strongly implies we are overfitting. Given the number of features, this is unsurprising..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x234357 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 115 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.transform(train[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may be able to improve this situation by increasing our \"regularization\" parameter -- this term allows us to penalize weights that become especially large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initially, we have 28469 records\n",
      "after deduping, we have 22372 records\n",
      "after removing <3 record categories, there are 22370 records\n",
      "training set has 16777 records -- 74.99776486365668 percent -- and test set has 5593 records\n",
      "in-sample accuracy:  0.9965428860940574\n",
      "test set accuracy:  0.9365278026104059\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_no_stopwords_stemmed</th>\n",
       "      <th>HS2</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [long_no_stopwords_stemmed, HS2, pred]\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "fu = FeatureUnion([('word_counts', CountVectorizer()), \n",
    "                   ('char_counts', CountVectorizer(analyzer='char', ngram_range=(2, 6)))], n_jobs=2)\n",
    "\n",
    "train, test = make_test_train(df, \"HS2\")\n",
    "\n",
    "# to do this, we just add the alpha parameter here \n",
    "# (this is because we defined alpha as a modifiable parameter above)\n",
    "clf, vec = make_model(train, fu, \"long_words_only\", \"HS2\", alpha=1e-3)\n",
    "y_test_true, y_test_pred = evaluate_model(clf, vec, train, test, \"long_words_only\", \"HS2\")\n",
    "\n",
    "\n",
    "\n",
    "test_reset = test.reset_index()\n",
    "test_reset[\"pred\"] = y_test_pred\n",
    "\n",
    "cols = [\"long_no_stopwords_stemmed\", \"HS2\", \"pred\"]\n",
    "CODE = \"29\"\n",
    "cond1 = (test_reset[\"HS2\"] == CODE) | (test_reset[\"pred\"] == CODE)\n",
    "cond2 = (test_reset[\"HS2\"] != test_reset[\"pred\"])\n",
    "cond3 = (test_reset[cols[0]].str.contains(\"racket\"))\n",
    "test_reset[cond1 & cond2 & cond3][cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks about the same, but we'll keep this in mind going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way we can deal with situations where certain words aren't being weighted heavily enough -- or others are being weighted too heavily -- by the model is to use a _feature scaling_ technique known as TF-IDF (term frequency - inverse document frequency). In essence, we're helping the model determine what features should be weighted and which ones should be ignored.\n",
    "\n",
    "The idea with TF-IDF is that instead of weighting each word with a 1 or 0, depending on whether or not it's in that particular record, instead we'll weight with more contextual information. There are many TF-IDF schemes, but they essentially all boil down to this:\n",
    "\n",
    "$$ \n",
    "\\frac{\\textrm{# times word occurs in record}}{\\textrm{# unique records the word occurs in}}\n",
    "$$\n",
    "\n",
    "In other words, the less frequently a word occurs across the entire set of descriptions, the more important it presumably is. Thus, since we frequently see the term, \"nesoi\",  for example, we'll weight that less.\n",
    "\n",
    "Let's go back to our baseline model, but using the `TfidfVectorizer` instead of the `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initially, we have 28469 records\n",
      "after deduping, we have 22372 records\n",
      "after removing <3 record categories, there are 22370 records\n",
      "training set has 16777 records -- 74.99776486365668 percent -- and test set has 5593 records\n",
      "in-sample accuracy:  0.8786433808189784\n",
      "test set accuracy:  0.8378330055426426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train, test = make_test_train(df, \"HS2\")\n",
    "# the only change to our \"formula\" is in the below line. See how easy this is?\n",
    "clf, vec = make_model(train, TfidfVectorizer(), \"long_no_stopwords_stemmed\", \"HS2\")\n",
    "y_test_true, y_test_pred = evaluate_model(clf, vec, train, test, \"long_no_stopwords_stemmed\", \"HS2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like in the case of this dataset, TF-IDF does worse across the board. As with other common techniques (e.g. stemming) this gives us a sign that every word included in these descriptions is important -- compared to other natural language datasets, these descriptions are quite sparse, so this makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it's fun and we set ourselves up to easily do so earlier, let's try one last thing before switching gears for a final important concept. We've been using logistic regression up until now. One of the nice things about SGD is that it's also easy to implement a performant _Linear Support Vector Machine_ (SVM), simply by changing the loss function that we are performing SGD to optimize. The SVM can't necessarily \"learn\" fundamentally different relationships than the logit classifier can -- both are linear -- however, the way that it optimizes its binary classifiers (\"maximum-margin\") can sometimes lead to slightly better results.\n",
    "\n",
    "Instead of 'log' loss, SGD uses 'hinge' loss. In the `make_model` function above, I included a parameter to modify the loss for ease-of-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initially, we have 28469 records\n",
      "after deduping, we have 22372 records\n",
      "after removing <3 record categories, there are 22370 records\n",
      "training set has 16777 records -- 74.99776486365668 percent -- and test set has 5593 records\n",
      "in-sample accuracy:  0.9976753889253144\n",
      "test set accuracy:  0.932415519399249\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_no_stopwords_stemmed</th>\n",
       "      <th>HS2</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [long_no_stopwords_stemmed, HS2, pred]\n",
       "Index: []"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fu = FeatureUnion([('word_counts', CountVectorizer()), \n",
    "                   ('char_counts', CountVectorizer(analyzer='char', ngram_range=(2, 6)))], n_jobs=2)\n",
    "train, test = make_test_train(df, \"HS2\")\n",
    "\n",
    "# only change!\n",
    "clf, vec = make_model(train, fu, \"long_words_only\", \"HS2\", loss='hinge')\n",
    "y_test_true, y_test_pred = evaluate_model(clf, vec, train, test, \"long_words_only\", \"HS2\")\n",
    "\n",
    "test_reset = test.reset_index()\n",
    "test_reset[\"pred\"] = y_test_pred\n",
    "\n",
    "cols = [\"long_no_stopwords_stemmed\", \"HS2\", \"pred\"]\n",
    "CODE = \"29\"\n",
    "cond1 = (test_reset[\"HS2\"] == CODE) | (test_reset[\"pred\"] == CODE)\n",
    "cond2 = (test_reset[\"HS2\"] != test_reset[\"pred\"])\n",
    "cond3 = (test_reset[cols[0]].str.contains(\"racket\"))\n",
    "test_reset[cond1 & cond2 & cond3][cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, we see the same results. \n",
    "\n",
    "At this point, you probably get the idea that it's pretty straightforward to go into the pandas/scikit-learn/nltk toolbox, pull out a technique, and plug it in to see how it improves your model. What if you want to do things a little bit more systematically, and make sure to squeeze every last drop of predictive power out of your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation and GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, we've looked at the \"art\" of feature extraction. We've seen a couple successful approaches and many unsuccessful ones to improve accuracy on our test set (this more-or-less reflects reality, most stuff you try doesn't work). However, we can actually systematize finding the best combination of tricks that we've tried by having the computer try every single combination of them. Scikit-learn calls this concept \"grid search\". \n",
    "\n",
    "If we did this with just a train/test set, we'd risk \"overfitting\" by simply finding the model that happens to work best on this particular test set. As such, while undertaking this process, we will further divide our training set into \"training\" and \"validation\" sets, which we'll randomly shuffle as we go along. Then, once we've determined the best model, we'll see how it performs on the test set we've been holding out this whole time.\n",
    "\n",
    "Above, we mentioned \"cross validation\" -- this is exactly that! Here's a handy diagram:\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*rgba1BIOUys7wQcXcL4U5A.png\" style=\"height: 400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do this cleanly, we'll need to get a bit fancier with scikit-learn as well. But, as with many things, scikit-learn does offer this functionality out-of-the-box, provided you write your code to conform to its interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/.miniconda2/envs/new_gpd_3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# pipelines are another useful abstraction provided by scikit-learn\n",
    "pipe = Pipeline([('features', CountVectorizer()),\n",
    "                  ('clf', SGDClassifier())])\n",
    "\n",
    "fu = FeatureUnion([('word_counts', CountVectorizer()), \n",
    "                   ('char_counts', CountVectorizer(analyzer='char', ngram_range=(2, 6)))])\n",
    "\n",
    "# this specifies that we will try all of the productive feature extraction techniques\n",
    "# including a combination we didn't try above: words, bigrams, and sub-character features\n",
    "possible_features = [CountVectorizer(ngram_range=(1, 2)),\n",
    "                     fu]\n",
    "\n",
    "# this now specifies all of the combinations of things we want to try\n",
    "# note we've included the log loss vs. hinge loss, a number of 'alpha' regularization terms,\n",
    "# as well as a couple of different regularization strategies, again just to give you an idea\n",
    "# of how simple it is to plug in more options\n",
    "params = [    {\n",
    "        'features': possible_features,\n",
    "        'clf__alpha': (1e-5, 1e-4, 1e-3, 1e-2),\n",
    "        'clf__penalty': ('l2',),\n",
    "        'clf__max_iter': (5000,),\n",
    "        'clf__loss': ('hinge', 'log')\n",
    "    }, ]\n",
    "\n",
    "# this takes a while\n",
    "gs = GridSearchCV(pipe, params, verbose=1, n_jobs=NCPUS)\n",
    "gs.fit(train[\"long_words_only\"], train[\"HS2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: some other random features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initially, we have 28469 records\n",
      "after deduping, we have 22372 records\n",
      "after removing <3 record categories, there are 22370 records\n",
      "training set has 16777 records -- 74.99776486365668 percent -- and test set has 5593 records\n",
      "in-sample accuracy:  0.996960123979\n",
      "test set accuracy:  0.932773109244\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_no_stopwords_stemmed</th>\n",
       "      <th>HS2</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [long_no_stopwords_stemmed, HS2, pred]\n",
       "Index: []"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FirstTwoVectorizer(CountVectorizer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        first_two = X.str.split().str[:2].apply(\" \".join)\n",
    "        super().fit(first_two, y)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "fu = FeatureUnion([('word_counts', CountVectorizer()), \n",
    "                   ('char_counts', CountVectorizer(analyzer='char', ngram_range=(2, 6))),\n",
    "                   ('first_two', FirstTwoVectorizer())], n_jobs=2, transformer_weights={'first_two': 2, 'word_counts': 1, 'char_counts': 1})\n",
    "train, test = make_test_train(df, \"long_no_stopwords\", \"HS2\")\n",
    "clf, vec = make_model(train, fu, \"long_no_stopwords\", \"HS2\")\n",
    "y_test_true, y_test_pred = evaluate_model(clf, vec, train, test, \"long_no_stopwords\", \"HS2\")\n",
    "\n",
    "test_reset = test.reset_index()\n",
    "test_reset[\"pred\"] = y_test_pred\n",
    "\n",
    "cols = [\"long_no_stopwords_stemmed\", \"HS2\", \"pred\"]\n",
    "CODE = \"29\"\n",
    "cond1 = (test_reset[\"HS2\"] == CODE) | (test_reset[\"pred\"] == CODE)\n",
    "cond2 = (test_reset[\"HS2\"] != test_reset[\"pred\"])\n",
    "cond3 = (test_reset[cols[0]].str.contains(\"racket\"))\n",
    "test_reset[cond1 & cond2 & cond3][cols]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
