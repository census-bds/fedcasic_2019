{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rapidly Prototyping a Machine Learning Pipeline\n",
    "\n",
    "The purpose of this workshop is to show how easy it is to take an idea and turn it into a successful Machine Learning application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: The idea/loading data\n",
    "In line with Tuesday's panel around \"autocoders,\" we'll demonstrate how to use text descriptions to predict codes. Along the way, we'll show some good practice when practicing machine learning. While this notebook will focus on coding products using descriptions, this approach should work for many scenarios where one has a dataset with both text descriptions and their associated codes.\n",
    "\n",
    "#### The data\n",
    "This data comes from Census' own Foreign Trade site: https://www.census.gov/foreign-trade/schedules/b/index.html#download\n",
    "\n",
    "Specifically, this is the \"concordance file\" for the Harmonized System's import/export codes. This gives us the code and a description of what products fit into that code: perfect for the sort of automatic coding we want to do. We've included the file here (in the `data/` subfolder) for ease-of-use.\n",
    "\n",
    "The import and export codes are slightly different at the 10-digit level; however, the codes are heirarchical and at the 6-digit level they are the same (as defined by an international standards group). We'll model at an even less granular level than that - the 4-digit level - based on the amount of data that we have.\n",
    "\n",
    "First, let's take a look in a text editor. Jupyter has one that's enough for this.\n",
    "\n",
    "From here, taking a look at the `imp-stru.txt` file will give us the \"schema.\"\n",
    "\n",
    "Now, we'll use a combination of python and pandas to ingest and clean this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 0\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we develop this by looking at the 'imp-stru.txt' file. the export structure is also the same\n",
    "def process_impexp_line(line, which='import'):\n",
    "    hs_code = line[:10]\n",
    "    short_desc = line[14:65]\n",
    "    long_desc = line[69:219]\n",
    "    return (hs_code, short_desc, long_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0101210010',\n",
       "  'HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE    ',\n",
       "  'HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                       '),\n",
       " ('0101210020',\n",
       "  'HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE  ',\n",
       "  'HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                     '),\n",
       " ('0101290010',\n",
       "  'HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI       ',\n",
       "  'HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                              ')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is one way of reading / processing a file in python\n",
    "with open(\"data/imp-code.txt\") as imp_f:\n",
    "    imp_lines = []\n",
    "    for line in imp_f:\n",
    "        imp_lines.append(process_impexp_line(line))\n",
    "        \n",
    "# let's take a look. Note all the extra whitespace. We'll have to remove that.\n",
    "imp_lines[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0101210000',\n",
       "  'HORSES, PUREBRED BREEDING, LIVE                    ',\n",
       "  'HORSES, PUREBRED BREEDING, LIVE                                                                                                                       '),\n",
       " ('0101290000',\n",
       "  'HORSES, LIVE, EXCEPT PUREBRED BREEDING             ',\n",
       "  'HORSES, LIVE, EXCEPT PUREBRED BREEDING                                                                                                                '),\n",
       " ('0101300000',\n",
       "  'ASSES, LIVE                                        ',\n",
       "  'ASSES, LIVE                                                                                                                                           ')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a slightly fancier way of doing the same thing\n",
    "with open(\"data/exp-code.txt\") as exp_f:\n",
    "    exp_lines = [process_impexp_line(line) for line in exp_f]\n",
    "        \n",
    "# always good to take a look at things along the way\n",
    "exp_lines[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got two lists (`imp_lines` and `exp_lines`) that hold all of the codes and descriptions, let's put them in a data frame and start processing/looking around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = imp_lines + exp_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101210010</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210020</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0                                                    1  \\\n",
       "0  0101210010  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE       \n",
       "1  0101210020  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE     \n",
       "\n",
       "                                                                                                                                                        2  \n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                         \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                       "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you can see in the output, pandas is smart enough to take a list of tuples\n",
    "# and turn it into a table in a \"sensible\" way. We do need to specify column names though.\n",
    "df = pd.DataFrame(all_lines)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df has 28469 rows\n",
      "df has 22660 unique HS 10-digit codes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS10</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>long_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101210010</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210020</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0101290010</td>\n",
       "      <td>HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI</td>\n",
       "      <td>HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101290090</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101300000</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         HS10                                           short_desc  \\\n",
       "0  0101210010  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE       \n",
       "1  0101210020  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE     \n",
       "2  0101290010  HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI          \n",
       "3  0101290090  HORSES, LIVE, NESOI                                   \n",
       "4  0101300000  ASSES, LIVE                                           \n",
       "\n",
       "                                                                                                                                                long_desc  \n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                         \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                       \n",
       "2  HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                \n",
       "3  HORSES, LIVE, NESOI                                                                                                                                     \n",
       "4  ASSES, LIVE                                                                                                                                             "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_lines, columns=['HS10', 'short_desc', 'long_desc'])\n",
    "print(\"df has\", len(df), \"rows\")\n",
    "print(\"df has\", df['HS10'].nunique(), 'unique HS 10-digit codes')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. We have something like 28,000 rows. As we can immediately see, there is some overlap between the 10-digit import and export codes, hence the only ~22,000 unique 10-digit codes. It is likely that duplicate codes have the same description in both files... and from the perspective of the model, 2 copies of the same description/code is just as good as 1, so we'll only want to retain one copy of those. Additionally, we need to standardize and process this text, which will probably leave us with more of duplicate text strings as well. Let's start processing and find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Cleaning / processing the data\n",
    "\n",
    "There are two main libraries for cleaning and processing text data in Python:\n",
    "- `spaCy`\n",
    "- `nltk`\n",
    "In this case, we'll use nltk, but spaCy is equally as good.\n",
    "\n",
    "Additionally, depending on the type of text that you have, there are many different ways to process and \"extract features\" (i.e. create variables for modelling) from that text. For example, if you're working with phrases/sentences, spaCy has good tools for determining which part-of-speech each word in a sentence maps to.\n",
    "\n",
    "In our case, we have very simple product descriptions (perhaps closer to \"tags\" than sentences), so less processing is required. We'll start out by using regular expressions. This is a good tutorial to learn more about those: https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS10</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>long_words_only</th>\n",
       "      <th>long_stripped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4458</th>\n",
       "      <td>2921193100</td>\n",
       "      <td>3-AMINO-3-METHYL-1-BUTYNE;2-CHLORO-N,N-DIMETHYL...</td>\n",
       "      <td>3-AMINO-3-METHYL-1-BUTYNE;2-CHLORO-N,N-DIMETHYL-ETHYLAMINE HYDROCHLORIDE; 2-(DIETHYLAMINO) ETHYLCHLORIDE HYDROCHLORIDE; AND DIMETHYLAMINOISOPROPYL...</td>\n",
       "      <td>3 amino 3 methyl 1 butyne 2 chloro n n dimethyl ethylamine hydrochloride 2 diethylamino ethylchloride hydrochloride and dimethylaminoisopropyl</td>\n",
       "      <td>3-AMINO-3-METHYL-1-BUTYNE;2-CHLORO-N,N-DIMETHYL-ETHYLAMINE HYDROCHLORIDE; 2-(DIETHYLAMINO) ETHYLCHLORIDE HYDROCHLORIDE; AND DIMETHYLAMINOISOPROPYL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5011</th>\n",
       "      <td>2937231020</td>\n",
       "      <td>PROGESTERONE OF ANIMAL OR VEGETABLE ORIGIN</td>\n",
       "      <td>PROGESTERONE OF ANIMAL OR VEGETABLE ORIGIN</td>\n",
       "      <td>progesterone of animal or vegetable origin</td>\n",
       "      <td>PROGESTERONE OF ANIMAL OR VEGETABLE ORIGIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23154</th>\n",
       "      <td>4818200020</td>\n",
       "      <td>TOWELS OF PAPER PULP/PAPER/CELLULOSE WADDING/WEBS</td>\n",
       "      <td>TOWELS OF PAPER PULP, PAPER, CELLULOSE WADDING OR WEBS OF CELLULOSE FIBERS</td>\n",
       "      <td>towels of paper pulp paper cellulose wadding or webs of cellulose fibers</td>\n",
       "      <td>TOWELS OF PAPER PULP, PAPER, CELLULOSE WADDING OR WEBS OF CELLULOSE FIBERS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             HS10                                           short_desc  \\\n",
       "4458   2921193100  3-AMINO-3-METHYL-1-BUTYNE;2-CHLORO-N,N-DIMETHYL...    \n",
       "5011   2937231020  PROGESTERONE OF ANIMAL OR VEGETABLE ORIGIN            \n",
       "23154  4818200020  TOWELS OF PAPER PULP/PAPER/CELLULOSE WADDING/WEBS     \n",
       "\n",
       "                                                                                                                                                    long_desc  \\\n",
       "4458   3-AMINO-3-METHYL-1-BUTYNE;2-CHLORO-N,N-DIMETHYL-ETHYLAMINE HYDROCHLORIDE; 2-(DIETHYLAMINO) ETHYLCHLORIDE HYDROCHLORIDE; AND DIMETHYLAMINOISOPROPYL...    \n",
       "5011   PROGESTERONE OF ANIMAL OR VEGETABLE ORIGIN                                                                                                               \n",
       "23154  TOWELS OF PAPER PULP, PAPER, CELLULOSE WADDING OR WEBS OF CELLULOSE FIBERS                                                                               \n",
       "\n",
       "                                                                                                                                      long_words_only  \\\n",
       "4458   3 amino 3 methyl 1 butyne 2 chloro n n dimethyl ethylamine hydrochloride 2 diethylamino ethylchloride hydrochloride and dimethylaminoisopropyl   \n",
       "5011   progesterone of animal or vegetable origin                                                                                                       \n",
       "23154  towels of paper pulp paper cellulose wadding or webs of cellulose fibers                                                                         \n",
       "\n",
       "                                                                                                                                               long_stripped  \n",
       "4458   3-AMINO-3-METHYL-1-BUTYNE;2-CHLORO-N,N-DIMETHYL-ETHYLAMINE HYDROCHLORIDE; 2-(DIETHYLAMINO) ETHYLCHLORIDE HYDROCHLORIDE; AND DIMETHYLAMINOISOPROPYL...  \n",
       "5011   PROGESTERONE OF ANIMAL OR VEGETABLE ORIGIN                                                                                                             \n",
       "23154  TOWELS OF PAPER PULP, PAPER, CELLULOSE WADDING OR WEBS OF CELLULOSE FIBERS                                                                             "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll do this step-by-step, to illustrate\n",
    "df['long_stripped'] = df['long_desc'].str.strip()\n",
    "\n",
    "# this is pulling 5 random rows, instead of the top 5, just for some variety\n",
    "# note the new colummn\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS10</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>long_words_only</th>\n",
       "      <th>long_stripped</th>\n",
       "      <th>long_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101210010</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>horses and asses purebred breeding male live</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>horses and asses, purebred breeding, male, live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210020</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>horses and asses purebred breeding female live</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>horses and asses, purebred breeding, female, live</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         HS10                                           short_desc  \\\n",
       "0  0101210010  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE       \n",
       "1  0101210020  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE     \n",
       "\n",
       "                                                                                                                                                long_desc  \\\n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                          \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                        \n",
       "\n",
       "                                  long_words_only  \\\n",
       "0  horses and asses purebred breeding male live     \n",
       "1  horses and asses purebred breeding female live   \n",
       "\n",
       "                                       long_stripped  \\\n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE     \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE   \n",
       "\n",
       "                                          long_lower  \n",
       "0  horses and asses, purebred breeding, male, live    \n",
       "1  horses and asses, purebred breeding, female, live  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['long_lower'] = df['long_stripped'].str.lower()\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS10</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>long_words_only</th>\n",
       "      <th>long_stripped</th>\n",
       "      <th>long_lower</th>\n",
       "      <th>long_word_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101210010</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>horses and asses purebred breeding male live</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>horses and asses, purebred breeding, male, live</td>\n",
       "      <td>[horses, and, asses, purebred, breeding, male, live]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210020</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>horses and asses purebred breeding female live</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>horses and asses, purebred breeding, female, live</td>\n",
       "      <td>[horses, and, asses, purebred, breeding, female, live]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0101290010</td>\n",
       "      <td>HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI</td>\n",
       "      <td>HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING</td>\n",
       "      <td>horses imported for immediate slaughter live except purebred breeding</td>\n",
       "      <td>HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING</td>\n",
       "      <td>horses, imported for immediate slaughter, live, except purebred breeding</td>\n",
       "      <td>[horses, imported, for, immediate, slaughter, live, except, purebred, breeding]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101290090</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "      <td>horses live nesoi</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "      <td>horses, live, nesoi</td>\n",
       "      <td>[horses, live, nesoi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101300000</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "      <td>asses live</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "      <td>asses, live</td>\n",
       "      <td>[asses, live]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         HS10                                           short_desc  \\\n",
       "0  0101210010  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE       \n",
       "1  0101210020  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE     \n",
       "2  0101290010  HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI          \n",
       "3  0101290090  HORSES, LIVE, NESOI                                   \n",
       "4  0101300000  ASSES, LIVE                                           \n",
       "\n",
       "                                                                                                                                                long_desc  \\\n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                          \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                        \n",
       "2  HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                 \n",
       "3  HORSES, LIVE, NESOI                                                                                                                                      \n",
       "4  ASSES, LIVE                                                                                                                                              \n",
       "\n",
       "                                                         long_words_only  \\\n",
       "0  horses and asses purebred breeding male live                            \n",
       "1  horses and asses purebred breeding female live                          \n",
       "2  horses imported for immediate slaughter live except purebred breeding   \n",
       "3  horses live nesoi                                                       \n",
       "4  asses live                                                              \n",
       "\n",
       "                                                              long_stripped  \\\n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                            \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                          \n",
       "2  HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING   \n",
       "3  HORSES, LIVE, NESOI                                                        \n",
       "4  ASSES, LIVE                                                                \n",
       "\n",
       "                                                                 long_lower  \\\n",
       "0  horses and asses, purebred breeding, male, live                            \n",
       "1  horses and asses, purebred breeding, female, live                          \n",
       "2  horses, imported for immediate slaughter, live, except purebred breeding   \n",
       "3  horses, live, nesoi                                                        \n",
       "4  asses, live                                                                \n",
       "\n",
       "                                                                    long_word_list  \n",
       "0  [horses, and, asses, purebred, breeding, male, live]                             \n",
       "1  [horses, and, asses, purebred, breeding, female, live]                           \n",
       "2  [horses, imported, for, immediate, slaughter, live, except, purebred, breeding]  \n",
       "3  [horses, live, nesoi]                                                            \n",
       "4  [asses, live]                                                                    "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# this is in general one of the more useful regular expressions to know\n",
    "# the '\\w' searches for word-like tokens, and the '+' says \"one or more\"\n",
    "# combined, this gets us 'one or more characters', i.e. word tokens w/o commas, spaces, etc.\n",
    "WORD_REGEX = r'\\w+'\n",
    "\n",
    "def find_words(desc):\n",
    "    return re.findall(WORD_REGEX, desc)\n",
    "df['long_word_list'] = df['long_lower'].apply(find_words)\n",
    "df['long_words_only'] = df['long_word_list'].str.join(' ')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean things up a bit, let's get rid of some of these intermediate columns.\n",
    "\n",
    "In practice, this is a useful thing to do because it will free up memory. If you find your code is using too much memory or is running very slow, then consider deleting unnecessary columns in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS10</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>long_words_only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101210010</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>horses and asses purebred breeding male live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210020</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>horses and asses purebred breeding female live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0101290010</td>\n",
       "      <td>HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI</td>\n",
       "      <td>HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING</td>\n",
       "      <td>horses imported for immediate slaughter live except purebred breeding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101290090</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "      <td>horses live nesoi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101300000</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "      <td>asses live</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         HS10                                           short_desc  \\\n",
       "0  0101210010  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE       \n",
       "1  0101210020  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE     \n",
       "2  0101290010  HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI          \n",
       "3  0101290090  HORSES, LIVE, NESOI                                   \n",
       "4  0101300000  ASSES, LIVE                                           \n",
       "\n",
       "                                                                                                                                                long_desc  \\\n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                          \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                        \n",
       "2  HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                 \n",
       "3  HORSES, LIVE, NESOI                                                                                                                                      \n",
       "4  ASSES, LIVE                                                                                                                                              \n",
       "\n",
       "                                                         long_words_only  \n",
       "0  horses and asses purebred breeding male live                           \n",
       "1  horses and asses purebred breeding female live                         \n",
       "2  horses imported for immediate slaughter live except purebred breeding  \n",
       "3  horses live nesoi                                                      \n",
       "4  asses live                                                             "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running this cell more than once will cause an error\n",
    "# because when you try to delete somethat that has \n",
    "# already been deleted, it's no longer there\n",
    "del df['long_stripped']\n",
    "del df['long_lower']\n",
    "del df['long_word_list']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Initial Model\n",
    "Now that we've gotten English words only, we can try a simple model. Let's get into the modelling approach and the package that we'll use to implement it, `scikit-learn`.\n",
    "\n",
    "#We'll be implementing a _bag-of-words_ model. The idea is very simple: each word becomes a separate variable. For each variable, the value is the number of times that word occurs in that particular record. Let's demonstrate with a quick example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    horses and asses purebred breeding male live                         \n",
       "1    horses and asses purebred breeding female live                       \n",
       "2    horses imported for immediate slaughter live except purebred breeding\n",
       "3    horses live nesoi                                                    \n",
       "4    asses live                                                           \n",
       "Name: long_words_only, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try this process on the first few and see what we get back\n",
    "first_few_only = df.head()\n",
    "# look at the final results, for reference\n",
    "first_few_only['long_words_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0],\n",
       "       [1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn calls this process \"vectorizing\", i.e. turning a sentence into a vector of variables.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "# this will actually convert our first few descriptions into vectors\n",
    "tfd = cv.fit_transform(first_few_only['long_words_only'])\n",
    "# by default, it's a sparse matrix\n",
    "tfd.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For explainability, let's cleanly present this mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>asses</th>\n",
       "      <th>breeding</th>\n",
       "      <th>except</th>\n",
       "      <th>female</th>\n",
       "      <th>for</th>\n",
       "      <th>horses</th>\n",
       "      <th>immediate</th>\n",
       "      <th>imported</th>\n",
       "      <th>live</th>\n",
       "      <th>male</th>\n",
       "      <th>nesoi</th>\n",
       "      <th>purebred</th>\n",
       "      <th>slaughter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_words_only</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>horses and asses purebred breeding male live</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horses and asses purebred breeding female live</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horses imported for immediate slaughter live except purebred breeding</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horses live nesoi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asses live</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       and  \\\n",
       "long_words_only                                                              \n",
       "horses and asses purebred breeding male live                           1     \n",
       "horses and asses purebred breeding female live                         1     \n",
       "horses imported for immediate slaughter live except purebred breeding  0     \n",
       "horses live nesoi                                                      0     \n",
       "asses live                                                             0     \n",
       "\n",
       "                                                                       asses  \\\n",
       "long_words_only                                                                \n",
       "horses and asses purebred breeding male live                           1       \n",
       "horses and asses purebred breeding female live                         1       \n",
       "horses imported for immediate slaughter live except purebred breeding  0       \n",
       "horses live nesoi                                                      0       \n",
       "asses live                                                             1       \n",
       "\n",
       "                                                                       breeding  \\\n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           1          \n",
       "horses and asses purebred breeding female live                         1          \n",
       "horses imported for immediate slaughter live except purebred breeding  1          \n",
       "horses live nesoi                                                      0          \n",
       "asses live                                                             0          \n",
       "\n",
       "                                                                       except  \\\n",
       "long_words_only                                                                 \n",
       "horses and asses purebred breeding male live                           0        \n",
       "horses and asses purebred breeding female live                         0        \n",
       "horses imported for immediate slaughter live except purebred breeding  1        \n",
       "horses live nesoi                                                      0        \n",
       "asses live                                                             0        \n",
       "\n",
       "                                                                       female  \\\n",
       "long_words_only                                                                 \n",
       "horses and asses purebred breeding male live                           0        \n",
       "horses and asses purebred breeding female live                         1        \n",
       "horses imported for immediate slaughter live except purebred breeding  0        \n",
       "horses live nesoi                                                      0        \n",
       "asses live                                                             0        \n",
       "\n",
       "                                                                       for  \\\n",
       "long_words_only                                                              \n",
       "horses and asses purebred breeding male live                           0     \n",
       "horses and asses purebred breeding female live                         0     \n",
       "horses imported for immediate slaughter live except purebred breeding  1     \n",
       "horses live nesoi                                                      0     \n",
       "asses live                                                             0     \n",
       "\n",
       "                                                                       horses  \\\n",
       "long_words_only                                                                 \n",
       "horses and asses purebred breeding male live                           1        \n",
       "horses and asses purebred breeding female live                         1        \n",
       "horses imported for immediate slaughter live except purebred breeding  1        \n",
       "horses live nesoi                                                      1        \n",
       "asses live                                                             0        \n",
       "\n",
       "                                                                       immediate  \\\n",
       "long_words_only                                                                    \n",
       "horses and asses purebred breeding male live                           0           \n",
       "horses and asses purebred breeding female live                         0           \n",
       "horses imported for immediate slaughter live except purebred breeding  1           \n",
       "horses live nesoi                                                      0           \n",
       "asses live                                                             0           \n",
       "\n",
       "                                                                       imported  \\\n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           0          \n",
       "horses and asses purebred breeding female live                         0          \n",
       "horses imported for immediate slaughter live except purebred breeding  1          \n",
       "horses live nesoi                                                      0          \n",
       "asses live                                                             0          \n",
       "\n",
       "                                                                       live  \\\n",
       "long_words_only                                                               \n",
       "horses and asses purebred breeding male live                           1      \n",
       "horses and asses purebred breeding female live                         1      \n",
       "horses imported for immediate slaughter live except purebred breeding  1      \n",
       "horses live nesoi                                                      1      \n",
       "asses live                                                             1      \n",
       "\n",
       "                                                                       male  \\\n",
       "long_words_only                                                               \n",
       "horses and asses purebred breeding male live                           1      \n",
       "horses and asses purebred breeding female live                         0      \n",
       "horses imported for immediate slaughter live except purebred breeding  0      \n",
       "horses live nesoi                                                      0      \n",
       "asses live                                                             0      \n",
       "\n",
       "                                                                       nesoi  \\\n",
       "long_words_only                                                                \n",
       "horses and asses purebred breeding male live                           0       \n",
       "horses and asses purebred breeding female live                         0       \n",
       "horses imported for immediate slaughter live except purebred breeding  0       \n",
       "horses live nesoi                                                      1       \n",
       "asses live                                                             0       \n",
       "\n",
       "                                                                       purebred  \\\n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           1          \n",
       "horses and asses purebred breeding female live                         1          \n",
       "horses imported for immediate slaughter live except purebred breeding  1          \n",
       "horses live nesoi                                                      0          \n",
       "asses live                                                             0          \n",
       "\n",
       "                                                                       slaughter  \n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           0          \n",
       "horses and asses purebred breeding female live                         0          \n",
       "horses imported for immediate slaughter live except purebred breeding  1          \n",
       "horses live nesoi                                                      0          \n",
       "asses live                                                             0          "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't worry about this, it's for pedagogical purposes\n",
    "columns = [x[0] for x in sorted(list(cv.vocabulary_.items()), key=lambda x: x[1])]\n",
    "pd.DataFrame(tfd.toarray(), columns=columns, index=first_few_only['long_words_only'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we get the idea, let's do this for the entire dataset and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<28469x12054 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 347471 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit_transform(df[\"long_words_only\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't try to convert this into an array as above, because it would be very memory-intensive. But we can see we have 12,054 unique words.\n",
    "\n",
    "Now, we have variables. But what what exactly are we going to model? The trade-off is that the more digits of the HS code we use, the fewer records there are within each code that the model can learn from. We aren't using any heirarchical information here, so each HS code is a completely separate category from the model's perspective. Let's extract 2-digit, 4-digit, and 6-digit harmonized codes in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1233.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.089213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>31.696155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>337.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               HS4\n",
       "count  1233.000000\n",
       "mean   23.089213  \n",
       "std    31.696155  \n",
       "min    1.000000   \n",
       "25%    6.000000   \n",
       "50%    12.000000  \n",
       "75%    27.000000  \n",
       "max    337.000000 "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"HS2\"] = df[\"HS10\"].str[:2]\n",
    "df[\"HS4\"] = df[\"HS10\"].str[:4]\n",
    "df[\"HS6\"] = df[\"HS10\"].str[:6]\n",
    "\n",
    "# we'll look at the distribution of # of records in each class at the 4-digit level.\n",
    "# Remember, we haven't de-duplicated, so this estimate is a bit high\n",
    "df.HS4.value_counts().hist()\n",
    "df.HS4.value_counts().describe().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some categories do have only 1 description. Let's see how many there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 codes with only 1 desc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9814    1\n",
       "8800    1\n",
       "Name: HS4, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcs = df.HS4.value_counts()\n",
    "print(len(vcs[vcs == 1]), \"codes with only 1 desc\")\n",
    "vcs[vcs == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd probably want to investigate these categories more closely. \n",
    "\n",
    "\n",
    "#### Training / test sets\n",
    "As a bare minimum, we need at least 2 records in any category we want to attempt to model. This is because we need to split our data into two pieces: the _training set_, which we'll develop the model on, and the _test set_, which we'll subsequently evaluate it on. We want to see how the model performs on descriptions it's never seen before.\n",
    "\n",
    "In practice, we almost certainly want more than 2, but we'll continue along here.\n",
    "\n",
    "In addition, [_cross validation_](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6), which we won't get into today, is an important technique in machine learning that prevents us from \"overfitting\" the model to the sample of data we're using. It's easy to do in python with `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after deduping, we have 22372 records\n",
      "after removing <2 record categories, there are 22372 records\n",
      "training set has 16779 records -- 75.0 percent -- and test set has 5593 records\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# first, let's remove duplicates\n",
    "deduped = df.drop_duplicates(subset=[\"long_words_only\"])\n",
    "print(\"after deduping, we have\", len(deduped), \"records\")\n",
    "\n",
    "#now, let's remove any HS4 category with <2 records\n",
    "vcs = deduped[\"HS2\"].value_counts()\n",
    "to_include = vcs[vcs > 1].index\n",
    "final_dataset = deduped[deduped.HS2.isin(to_include)]\n",
    "print(\"after removing <2 record categories, there are\", len(final_dataset), \"records\")\n",
    "\n",
    "# the stratify is important -- \n",
    "# it's making sure that we have an instance of each category in both the train and test sets\n",
    "\n",
    "train, test = train_test_split(final_dataset, stratify=final_dataset[\"HS2\"])\n",
    "print(\"training set has\", len(train), \"records --\", 100 * len(train) / len(final_dataset), \n",
    "      \"percent -- and test set has\", len(test), \"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've split up our data, we can choose a classifier. We'll use one of the simplest out there: Logistic Regression, often known as \"logit.\" Normally, logistic regression is a binary classifier. In our case, because we're categorizing something like 1200 codes, we'll actually be training 1200 models, and selecting the highest-probability prediction. This is known as \"one-vs-all\". There are other voting schemes to convert binary classifiers into multi-class classifiers.\n",
    "\n",
    "One more trick: instead of using the closed-form logistic regression classifier, we'll use a heuristic optimization approach that runs much more quickly and efficiently, called \"stochastic gradient descent.\" SGD, as it's abbreviated, is part of the core technique used to optimize neural nets (back-propagation or \"backprop\") as well. We'll leave it to you to convince yourself that this approach is just as good. You can get into the math or just try it out in Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/.miniconda2/envs/new_gpd_3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(train[\"long_words_only\"])\n",
    "y = train[\"HS2\"]\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained model in the `clf` variable. Let's see how it does on a simple metric: overall accuracy. That is, \"out of every code the model predicted, what fraction did it get right?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial accuracy:  0.9274092615769712\n"
     ]
    }
   ],
   "source": [
    "X_test = cv.transform(test[\"long_words_only\"])\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_test_true = test[\"HS2\"]\n",
    "print(\"initial accuracy: \", (y_test_pred == y_test_true).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Now, we can drill down a bit in a few ways. A productive way to do so is to look at failing codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.930777</td>\n",
       "      <td>0.914226</td>\n",
       "      <td>0.947939</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.985549</td>\n",
       "      <td>0.985549</td>\n",
       "      <td>0.985549</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.845528</td>\n",
       "      <td>0.762836</td>\n",
       "      <td>0.948328</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.912801</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.914498</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.989858</td>\n",
       "      <td>0.995918</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.982036</td>\n",
       "      <td>0.970414</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03</th>\n",
       "      <td>0.977636</td>\n",
       "      <td>0.987097</td>\n",
       "      <td>0.968354</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.993671</td>\n",
       "      <td>0.987421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.854839</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.990741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981651</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.931373</td>\n",
       "      <td>0.969388</td>\n",
       "      <td>0.896226</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.773481</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.872093</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.842697</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.948052</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04</th>\n",
       "      <td>0.915493</td>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.878378</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.971014</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.985294</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.962264</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08</th>\n",
       "      <td>0.957983</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.945455</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f1-score  precision    recall  support\n",
       "84  0.930777  0.914226   0.947939  461    \n",
       "62  0.985549  0.985549   0.985549  346    \n",
       "29  0.845528  0.762836   0.948328  329    \n",
       "85  0.912801  0.911111   0.914498  269    \n",
       "61  0.989858  0.995918   0.983871  248    \n",
       "73  0.976190  0.982036   0.970414  169    \n",
       "72  0.975309  0.987500   0.963415  164    \n",
       "03  0.977636  0.987097   0.968354  158    \n",
       "52  0.993671  0.987421   1.000000  157    \n",
       "44  0.979021  0.972222   0.985915  142    \n",
       "90  0.854839  0.883333   0.828125  128    \n",
       "55  0.991597  0.983333   1.000000  118    \n",
       "64  0.990741  1.000000   0.981651  109    \n",
       "87  0.931373  0.969388   0.896226  106    \n",
       "91  1.000000  1.000000   1.000000  104    \n",
       "48  0.989011  0.989011   0.989011  91     \n",
       "28  0.773481  0.769231   0.777778  90     \n",
       "39  0.872093  0.903614   0.842697  89     \n",
       "07  0.952381  0.963855   0.941176  85     \n",
       "54  0.987654  0.987654   0.987654  81     \n",
       "20  0.948052  0.973333   0.924051  79     \n",
       "04  0.915493  0.955882   0.878378  74     \n",
       "63  0.956522  0.956522   0.956522  69     \n",
       "70  0.971014  0.957143   0.985294  68     \n",
       "94  0.857143  0.810811   0.909091  66     \n",
       "82  0.920354  0.962963   0.881356  59     \n",
       "40  0.918919  0.962264   0.879310  58     \n",
       "08  0.957983  0.919355   1.000000  57     \n",
       "24  0.964912  0.964912   0.964912  57     \n",
       "16  0.945455  0.962963   0.928571  56     "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "rep = (classification_report(y_test_true, y_test_pred, output_dict=True))\n",
    "sorted_by_support = sorted(rep.items(), key=lambda x: x[1]['support'], reverse=True)\n",
    "pd.DataFrame((s[1] for s in sorted_by_support), index=(s[0] for s in sorted_by_support))[3:].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see code 2933 is performing poorly. Let's investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reset = test.reset_index()\n",
    "test_reset[\"pred\"] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_words_only</th>\n",
       "      <th>HS4</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>confections or sweetmeats ready for consumption containing peanuts peanut butter or peanut paste put up for retail sale</td>\n",
       "      <td>1704</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>mixtures of fruits nuts and other edible part of plants prepared cereal products nesoi whether containing sugar other sweetened matter or spirit</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>banana pulp prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>pecans prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>lemons citrus limon citrus limonum prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>coconuts prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>crabs prepared or preserved nesoi</td>\n",
       "      <td>1605</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>brazil nuts prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>clams prepared or preserved nesoi</td>\n",
       "      <td>1605</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>nuts and other seeds nesoi prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>grapes prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>fruit nuts and other edible parts of plants prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>citrus fruit nesoi otherwise prepared or preserved whether or not containing sweetening or spirit nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2603</th>\n",
       "      <td>citron prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>almonds prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>plums including prune plums and sloes prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>0809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3186</th>\n",
       "      <td>avocados prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>watermelon seeds prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3235</th>\n",
       "      <td>octopus prepared or preserved nesoi</td>\n",
       "      <td>1605</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>blueberries except canned wild blueberries prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3469</th>\n",
       "      <td>citrus fruit including bergamots nesoi prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>wild blueberries canned</td>\n",
       "      <td>2008</td>\n",
       "      <td>0810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4184</th>\n",
       "      <td>peanut butter and paste described in general note 15 of the tariff schedule and entered pursuant to its provisions</td>\n",
       "      <td>2008</td>\n",
       "      <td>0405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>fruit mixtures containing peaches or pears prepared nesoi packed in a liquid medium in airtight containers each holding over 1 4 kg</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4383</th>\n",
       "      <td>orange pulp prepared or preserved nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4401</th>\n",
       "      <td>blanched peanuts nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4665</th>\n",
       "      <td>anchovies prepared or preserved nesoi</td>\n",
       "      <td>1604</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4812</th>\n",
       "      <td>crustaceans prepared or preserved nesoi</td>\n",
       "      <td>1605</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>mixtures of fruits nuts edible parts of plants prepared cereal products in airtight containers no apricots citrus fruits peaches or pears</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5276</th>\n",
       "      <td>fruit mixtures containing oranges or grapefruit prepared nesoi packed in a liquid medium in airtight containers</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5456</th>\n",
       "      <td>satsumas prepared or preserved in airtight containers for an aggregate quantity not to exceed 40000 metric tons entered in any year nesoi</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                       long_words_only  \\\n",
       "247   confections or sweetmeats ready for consumption containing peanuts peanut butter or peanut paste put up for retail sale                            \n",
       "324   mixtures of fruits nuts and other edible part of plants prepared cereal products nesoi whether containing sugar other sweetened matter or spirit   \n",
       "547   banana pulp prepared or preserved nesoi                                                                                                            \n",
       "583   pecans prepared or preserved nesoi                                                                                                                 \n",
       "770   lemons citrus limon citrus limonum prepared or preserved nesoi                                                                                     \n",
       "1020  coconuts prepared or preserved nesoi                                                                                                               \n",
       "1164  crabs prepared or preserved nesoi                                                                                                                  \n",
       "1315  brazil nuts prepared or preserved nesoi                                                                                                            \n",
       "1417  clams prepared or preserved nesoi                                                                                                                  \n",
       "1879  nuts and other seeds nesoi prepared or preserved nesoi                                                                                             \n",
       "1907  grapes prepared or preserved nesoi                                                                                                                 \n",
       "1961  fruit nuts and other edible parts of plants prepared or preserved nesoi                                                                            \n",
       "2062  citrus fruit nesoi otherwise prepared or preserved whether or not containing sweetening or spirit nesoi                                            \n",
       "2603  citron prepared or preserved nesoi                                                                                                                 \n",
       "2799  almonds prepared or preserved nesoi                                                                                                                \n",
       "2990  plums including prune plums and sloes prepared or preserved nesoi                                                                                  \n",
       "3186  avocados prepared or preserved nesoi                                                                                                               \n",
       "3217  watermelon seeds prepared or preserved nesoi                                                                                                       \n",
       "3235  octopus prepared or preserved nesoi                                                                                                                \n",
       "3425  blueberries except canned wild blueberries prepared or preserved nesoi                                                                             \n",
       "3469  citrus fruit including bergamots nesoi prepared or preserved nesoi                                                                                 \n",
       "3731  wild blueberries canned                                                                                                                            \n",
       "4184  peanut butter and paste described in general note 15 of the tariff schedule and entered pursuant to its provisions                                 \n",
       "4211  fruit mixtures containing peaches or pears prepared nesoi packed in a liquid medium in airtight containers each holding over 1 4 kg                \n",
       "4383  orange pulp prepared or preserved nesoi                                                                                                            \n",
       "4401  blanched peanuts nesoi                                                                                                                             \n",
       "4665  anchovies prepared or preserved nesoi                                                                                                              \n",
       "4812  crustaceans prepared or preserved nesoi                                                                                                            \n",
       "5108  mixtures of fruits nuts edible parts of plants prepared cereal products in airtight containers no apricots citrus fruits peaches or pears          \n",
       "5276  fruit mixtures containing oranges or grapefruit prepared nesoi packed in a liquid medium in airtight containers                                    \n",
       "5456  satsumas prepared or preserved in airtight containers for an aggregate quantity not to exceed 40000 metric tons entered in any year nesoi          \n",
       "\n",
       "       HS4  pred  \n",
       "247   1704  2008  \n",
       "324   2008  2008  \n",
       "547   2008  2008  \n",
       "583   2008  2008  \n",
       "770   2008  2008  \n",
       "1020  2008  2008  \n",
       "1164  1605  2008  \n",
       "1315  2008  2008  \n",
       "1417  1605  2008  \n",
       "1879  2008  2008  \n",
       "1907  2008  2008  \n",
       "1961  2008  2008  \n",
       "2062  2008  2008  \n",
       "2603  2008  2008  \n",
       "2799  2008  2008  \n",
       "2990  2008  0809  \n",
       "3186  2008  2008  \n",
       "3217  2008  2008  \n",
       "3235  1605  2008  \n",
       "3425  2008  2008  \n",
       "3469  2008  2008  \n",
       "3731  2008  0810  \n",
       "4184  2008  0405  \n",
       "4211  2008  2008  \n",
       "4383  2008  2008  \n",
       "4401  2008  2008  \n",
       "4665  1604  2008  \n",
       "4812  1605  2008  \n",
       "5108  2008  2008  \n",
       "5276  2008  2008  \n",
       "5456  2008  2008  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"long_words_only\", \"HS4\", \"pred\"]\n",
    "CODE = \"2008\"\n",
    "test_reset[(test_reset[\"HS4\"] == CODE) | (test_reset[\"pred\"] == CODE)][cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here? We can see, for example, that code 1605 \"octopus prepared or preserved nesoi\" was mis-predicted into this class. It may well be because of the \"prepared or preserved nesoi\" bit. The model may be incorrectly attributing this to an important part of the 2008 code. \n",
    "\n",
    "We can deal with this using a _feature scaling_ technique known as TF-IDF (term frequency - inverse document frequency). In essence, we're helping the model determine what features should be weighted and which ones should be ignored.\n",
    "\n",
    "The idea with TF-IDF is that instead of weighting each word with a 1 or 0, depending on whether or not it's in that particular record, instead we'll weight with more contextual information. There are many TF-IDF schemes, but they essentially all boil down to this:\n",
    "\n",
    "$$ \n",
    "\\frac{\\textrm{# times word occurs in record}}{\\textrm{# unique records the word occurs in}}\n",
    "$$\n",
    "\n",
    "In other words, the less frequently a word occurs across the entire set of descriptions, the more important it presumably is. Thus, since we frequently see the terms, \"nesoi\", \"prepared\", and \"preserved\",  for example, we'll weight those less\n",
    "\n",
    "Let's look at the same example with the `CountVectorizer` as above, but using the `TfidfVectorizer` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>asses</th>\n",
       "      <th>breeding</th>\n",
       "      <th>except</th>\n",
       "      <th>female</th>\n",
       "      <th>for</th>\n",
       "      <th>horses</th>\n",
       "      <th>immediate</th>\n",
       "      <th>imported</th>\n",
       "      <th>live</th>\n",
       "      <th>male</th>\n",
       "      <th>nesoi</th>\n",
       "      <th>purebred</th>\n",
       "      <th>slaughter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_words_only</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>horses and asses purebred breeding male live</th>\n",
       "      <td>0.428751</td>\n",
       "      <td>0.355902</td>\n",
       "      <td>0.355902</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.299396</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.253227</td>\n",
       "      <td>0.531425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355902</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horses and asses purebred breeding female live</th>\n",
       "      <td>0.428751</td>\n",
       "      <td>0.355902</td>\n",
       "      <td>0.355902</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.531425</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.299396</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.253227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355902</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horses imported for immediate slaughter live except purebred breeding</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263873</td>\n",
       "      <td>0.39401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.39401</td>\n",
       "      <td>0.221978</td>\n",
       "      <td>0.39401</td>\n",
       "      <td>0.39401</td>\n",
       "      <td>0.187748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263873</td>\n",
       "      <td>0.39401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horses live nesoi</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.453331</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.383424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.804659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asses live</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.579739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            and  \\\n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           0.428751   \n",
       "horses and asses purebred breeding female live                         0.428751   \n",
       "horses imported for immediate slaughter live except purebred breeding  0.000000   \n",
       "horses live nesoi                                                      0.000000   \n",
       "asses live                                                             0.000000   \n",
       "\n",
       "                                                                          asses  \\\n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           0.355902   \n",
       "horses and asses purebred breeding female live                         0.355902   \n",
       "horses imported for immediate slaughter live except purebred breeding  0.000000   \n",
       "horses live nesoi                                                      0.000000   \n",
       "asses live                                                             0.814802   \n",
       "\n",
       "                                                                       breeding  \\\n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           0.355902   \n",
       "horses and asses purebred breeding female live                         0.355902   \n",
       "horses imported for immediate slaughter live except purebred breeding  0.263873   \n",
       "horses live nesoi                                                      0.000000   \n",
       "asses live                                                             0.000000   \n",
       "\n",
       "                                                                        except  \\\n",
       "long_words_only                                                                  \n",
       "horses and asses purebred breeding male live                           0.00000   \n",
       "horses and asses purebred breeding female live                         0.00000   \n",
       "horses imported for immediate slaughter live except purebred breeding  0.39401   \n",
       "horses live nesoi                                                      0.00000   \n",
       "asses live                                                             0.00000   \n",
       "\n",
       "                                                                         female  \\\n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           0.000000   \n",
       "horses and asses purebred breeding female live                         0.531425   \n",
       "horses imported for immediate slaughter live except purebred breeding  0.000000   \n",
       "horses live nesoi                                                      0.000000   \n",
       "asses live                                                             0.000000   \n",
       "\n",
       "                                                                           for  \\\n",
       "long_words_only                                                                  \n",
       "horses and asses purebred breeding male live                           0.00000   \n",
       "horses and asses purebred breeding female live                         0.00000   \n",
       "horses imported for immediate slaughter live except purebred breeding  0.39401   \n",
       "horses live nesoi                                                      0.00000   \n",
       "asses live                                                             0.00000   \n",
       "\n",
       "                                                                         horses  \\\n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           0.299396   \n",
       "horses and asses purebred breeding female live                         0.299396   \n",
       "horses imported for immediate slaughter live except purebred breeding  0.221978   \n",
       "horses live nesoi                                                      0.453331   \n",
       "asses live                                                             0.000000   \n",
       "\n",
       "                                                                       immediate  \\\n",
       "long_words_only                                                                    \n",
       "horses and asses purebred breeding male live                           0.00000     \n",
       "horses and asses purebred breeding female live                         0.00000     \n",
       "horses imported for immediate slaughter live except purebred breeding  0.39401     \n",
       "horses live nesoi                                                      0.00000     \n",
       "asses live                                                             0.00000     \n",
       "\n",
       "                                                                       imported  \\\n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           0.00000    \n",
       "horses and asses purebred breeding female live                         0.00000    \n",
       "horses imported for immediate slaughter live except purebred breeding  0.39401    \n",
       "horses live nesoi                                                      0.00000    \n",
       "asses live                                                             0.00000    \n",
       "\n",
       "                                                                           live  \\\n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           0.253227   \n",
       "horses and asses purebred breeding female live                         0.253227   \n",
       "horses imported for immediate slaughter live except purebred breeding  0.187748   \n",
       "horses live nesoi                                                      0.383424   \n",
       "asses live                                                             0.579739   \n",
       "\n",
       "                                                                           male  \\\n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           0.531425   \n",
       "horses and asses purebred breeding female live                         0.000000   \n",
       "horses imported for immediate slaughter live except purebred breeding  0.000000   \n",
       "horses live nesoi                                                      0.000000   \n",
       "asses live                                                             0.000000   \n",
       "\n",
       "                                                                          nesoi  \\\n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           0.000000   \n",
       "horses and asses purebred breeding female live                         0.000000   \n",
       "horses imported for immediate slaughter live except purebred breeding  0.000000   \n",
       "horses live nesoi                                                      0.804659   \n",
       "asses live                                                             0.000000   \n",
       "\n",
       "                                                                       purebred  \\\n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           0.355902   \n",
       "horses and asses purebred breeding female live                         0.355902   \n",
       "horses imported for immediate slaughter live except purebred breeding  0.263873   \n",
       "horses live nesoi                                                      0.000000   \n",
       "asses live                                                             0.000000   \n",
       "\n",
       "                                                                       slaughter  \n",
       "long_words_only                                                                   \n",
       "horses and asses purebred breeding male live                           0.00000    \n",
       "horses and asses purebred breeding female live                         0.00000    \n",
       "horses imported for immediate slaughter live except purebred breeding  0.39401    \n",
       "horses live nesoi                                                      0.00000    \n",
       "asses live                                                             0.00000    "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "# this will actually convert our first few descriptions into vectors\n",
    "tfd = vec.fit_transform(first_few_only['long_words_only'])\n",
    "# by default, it's a sparse matrix\n",
    "tfd.toarray()\n",
    "# don't worry about this, it's for pedagogical purposes\n",
    "columns = [x[0] for x in sorted(list(vec.vocabulary_.items()), key=lambda x: x[1])]\n",
    "pd.DataFrame(tfd.toarray(), columns=columns, index=first_few_only['long_words_only'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, without further ado, let's try a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/.miniconda2/envs/new_gpd_3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(train[\"long_words_only\"])\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X, train[\"HS2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial accuracy:  0.8371178258537457\n"
     ]
    }
   ],
   "source": [
    "X_test = cv.transform(test[\"long_words_only\"])\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_test_true = test[\"HS2\"]\n",
    "print(\"initial accuracy: \", (y_test_pred == y_test_true).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/christian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS10</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>long_words_only</th>\n",
       "      <th>HS2</th>\n",
       "      <th>HS4</th>\n",
       "      <th>HS6</th>\n",
       "      <th>long_no_stopwords</th>\n",
       "      <th>long_no_stopwords_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101210010</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE</td>\n",
       "      <td>horses and asses purebred breeding male live</td>\n",
       "      <td>01</td>\n",
       "      <td>0101</td>\n",
       "      <td>010121</td>\n",
       "      <td>horses asses purebred breeding male live</td>\n",
       "      <td>horses asses purebred breeding male l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210020</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE</td>\n",
       "      <td>horses and asses purebred breeding female live</td>\n",
       "      <td>01</td>\n",
       "      <td>0101</td>\n",
       "      <td>010121</td>\n",
       "      <td>horses asses purebred breeding female live</td>\n",
       "      <td>horses asses purebred breeding female l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0101290010</td>\n",
       "      <td>HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI</td>\n",
       "      <td>HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING</td>\n",
       "      <td>horses imported for immediate slaughter live except purebred breeding</td>\n",
       "      <td>01</td>\n",
       "      <td>0101</td>\n",
       "      <td>010129</td>\n",
       "      <td>horses imported immediate slaughter live except purebred breeding</td>\n",
       "      <td>horses imported immediate slaughter live except purebred breed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101290090</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "      <td>HORSES, LIVE, NESOI</td>\n",
       "      <td>horses live nesoi</td>\n",
       "      <td>01</td>\n",
       "      <td>0101</td>\n",
       "      <td>010129</td>\n",
       "      <td>horses live</td>\n",
       "      <td>horses l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101300000</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "      <td>ASSES, LIVE</td>\n",
       "      <td>asses live</td>\n",
       "      <td>01</td>\n",
       "      <td>0101</td>\n",
       "      <td>010130</td>\n",
       "      <td>asses live</td>\n",
       "      <td>asses l</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         HS10                                           short_desc  \\\n",
       "0  0101210010  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE       \n",
       "1  0101210020  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE     \n",
       "2  0101290010  HORSES, FOR IMMEDIATE SLAUGHTER, LIVE, NESOI          \n",
       "3  0101290090  HORSES, LIVE, NESOI                                   \n",
       "4  0101300000  ASSES, LIVE                                           \n",
       "\n",
       "                                                                                                                                                long_desc  \\\n",
       "0  HORSES AND ASSES, PUREBRED BREEDING, MALE, LIVE                                                                                                          \n",
       "1  HORSES AND ASSES, PUREBRED BREEDING, FEMALE, LIVE                                                                                                        \n",
       "2  HORSES, IMPORTED FOR IMMEDIATE SLAUGHTER, LIVE, EXCEPT PUREBRED BREEDING                                                                                 \n",
       "3  HORSES, LIVE, NESOI                                                                                                                                      \n",
       "4  ASSES, LIVE                                                                                                                                              \n",
       "\n",
       "                                                         long_words_only HS2  \\\n",
       "0  horses and asses purebred breeding male live                           01   \n",
       "1  horses and asses purebred breeding female live                         01   \n",
       "2  horses imported for immediate slaughter live except purebred breeding  01   \n",
       "3  horses live nesoi                                                      01   \n",
       "4  asses live                                                             01   \n",
       "\n",
       "    HS4     HS6  \\\n",
       "0  0101  010121   \n",
       "1  0101  010121   \n",
       "2  0101  010129   \n",
       "3  0101  010129   \n",
       "4  0101  010130   \n",
       "\n",
       "                                                   long_no_stopwords  \\\n",
       "0  horses asses purebred breeding male live                            \n",
       "1  horses asses purebred breeding female live                          \n",
       "2  horses imported immediate slaughter live except purebred breeding   \n",
       "3  horses live                                                         \n",
       "4  asses live                                                          \n",
       "\n",
       "                                        long_no_stopwords_stemmed  \n",
       "0  horses asses purebred breeding male l                           \n",
       "1  horses asses purebred breeding female l                         \n",
       "2  horses imported immediate slaughter live except purebred breed  \n",
       "3  horses l                                                        \n",
       "4  asses l                                                         "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "all_stopwords = set(stopwords.words('english'))\n",
    "all_stopwords.add('nesoi')\n",
    "all_stopwords.add('prepared')\n",
    "all_stopwords.add('preserved')\n",
    "def remove_stopwords(desc):\n",
    "    return \" \".join(d for d in desc.split() if d not in all_stopwords)\n",
    "df[\"long_no_stopwords\"] = df[\"long_words_only\"].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after deduping, we have 21293 records\n",
      "after removing <2 record categories, there are 21293 records\n",
      "training set has 15969 records -- 74.99647771568121 percent -- and test set has 5324 records\n"
     ]
    }
   ],
   "source": [
    "deduped = df.drop_duplicates(subset=[\"long_no_stopwords\"])\n",
    "print(\"after deduping, we have\", len(deduped), \"records\")\n",
    "\n",
    "#now, let's remove any HS4 category with <2 records\n",
    "vcs = deduped[\"HS2\"].value_counts()\n",
    "to_include = vcs[vcs > 1].index\n",
    "final_dataset = deduped[deduped.HS2.isin(to_include)]\n",
    "print(\"after removing <2 record categories, there are\", len(final_dataset), \"records\")\n",
    "\n",
    "# the stratify is important -- \n",
    "# it's making sure that we have an instance of each category in both the train and test sets\n",
    "\n",
    "train, test = train_test_split(final_dataset, stratify=final_dataset[\"HS2\"])\n",
    "print(\"training set has\", len(train), \"records --\", 100 * len(train) / len(final_dataset), \n",
    "      \"percent -- and test set has\", len(test), \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/.miniconda2/envs/new_gpd_3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial accuracy:  0.9256198347107438\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(train[\"long_no_stopwords\"])\n",
    "y = train[\"HS2\"]\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "X_test = cv.transform(test[\"long_no_stopwords\"])\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_test_true = test[\"HS2\"]\n",
    "print(\"initial accuracy: \", (y_test_pred == y_test_true).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "df[\"long_no_stopwords_stemmed\"] = df[\"long_no_stopwords\"].apply(stemmer.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after deduping, we have 21254 records\n",
      "after removing <2 record categories, there are 21254 records\n",
      "training set has 15940 records -- 74.99764750164675 percent -- and test set has 5314 records\n"
     ]
    }
   ],
   "source": [
    "deduped = df.drop_duplicates(subset=[\"long_no_stopwords_stemmed\"])\n",
    "print(\"after deduping, we have\", len(deduped), \"records\")\n",
    "\n",
    "#now, let's remove any HS4 category with <2 records\n",
    "vcs = deduped[\"HS2\"].value_counts()\n",
    "to_include = vcs[vcs > 1].index\n",
    "final_dataset = deduped[deduped.HS2.isin(to_include)]\n",
    "print(\"after removing <2 record categories, there are\", len(final_dataset), \"records\")\n",
    "\n",
    "# the stratify is important -- \n",
    "# it's making sure that we have an instance of each category in both the train and test sets\n",
    "\n",
    "train, test = train_test_split(final_dataset, stratify=final_dataset[\"HS2\"])\n",
    "print(\"training set has\", len(train), \"records --\", 100 * len(train) / len(final_dataset), \n",
    "      \"percent -- and test set has\", len(test), \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/.miniconda2/envs/new_gpd_3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial accuracy:  0.922092585622883\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(train[\"long_no_stopwords_stemmed\"])\n",
    "y = train[\"HS2\"]\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "X_test = cv.transform(test[\"long_no_stopwords_stemmed\"])\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_test_true = test[\"HS2\"]\n",
    "print(\"initial accuracy: \", (y_test_pred == y_test_true).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/.miniconda2/envs/new_gpd_3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial accuracy:  0.9190816710575838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(train[\"long_no_stopwords_stemmed\"])\n",
    "y = train[\"HS2\"]\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "X_test = cv.transform(test[\"long_no_stopwords_stemmed\"])\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_test_true = test[\"HS2\"]\n",
    "print(\"initial accuracy: \", (y_test_pred == y_test_true).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial accuracy:  0.8831388784343245\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = rf.predict(X_test)\n",
    "y_test_true = test[\"HS2\"]\n",
    "print(\"initial accuracy: \", (y_test_pred == y_test_true).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
